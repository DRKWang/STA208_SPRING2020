{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "from tensorflow.keras import models,layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from tensorflow import keras\n",
    "import DNN_softmax_and_SVM\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "#load and preprocess the data \n",
    "images = np.load('mnist.npz')\n",
    "x_train, x_test, y_train, y_test = images[\"x_train\"], images[\"x_test\"], images[\"y_train\"], images[\"y_test\"]\n",
    "\n",
    "# (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = tf.reshape(x_train,[60000,784])\n",
    "noise = np.random.normal(0, 1, x_train.shape)\n",
    "x_train+=noise    #add Gaussian noise to the training data\n",
    "x_test = tf.reshape(x_test,[10000,784])\n",
    "\n",
    "#do the PCA\n",
    "pca = PCA(n_components=70)\n",
    "x_train = pca.fit_transform(x_train)\n",
    "x_test = pca.transform(x_test)\n",
    "\n",
    "x_train, x_test = np.array(x_train, np.float32), np.array(x_test, np.float32)\n",
    "# Normalize images value from [0, 255] to [0, 1].\n",
    "x_train, x_test = x_train / 255., x_test / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 100, loss: 153.336853, accuracy: 0.845000\n",
      "step: 200, loss: 100.459839, accuracy: 0.850000\n",
      "step: 300, loss: 75.561409, accuracy: 0.885000\n",
      "step: 400, loss: 90.861603, accuracy: 0.845000\n",
      "step: 500, loss: 95.966957, accuracy: 0.870000\n",
      "step: 600, loss: 63.838150, accuracy: 0.890000\n",
      "step: 700, loss: 79.300247, accuracy: 0.885000\n",
      "step: 800, loss: 64.463196, accuracy: 0.880000\n",
      "step: 900, loss: 61.373005, accuracy: 0.900000\n",
      "step: 1000, loss: 69.103897, accuracy: 0.885000\n",
      "step: 1100, loss: 67.648438, accuracy: 0.885000\n",
      "step: 1200, loss: 83.173500, accuracy: 0.880000\n",
      "step: 1300, loss: 68.766098, accuracy: 0.895000\n",
      "step: 1400, loss: 74.581772, accuracy: 0.880000\n",
      "step: 1500, loss: 40.743233, accuracy: 0.935000\n",
      "step: 1600, loss: 45.050014, accuracy: 0.910000\n",
      "step: 1700, loss: 50.758247, accuracy: 0.925000\n",
      "step: 1800, loss: 41.141151, accuracy: 0.935000\n",
      "step: 1900, loss: 59.358784, accuracy: 0.900000\n",
      "step: 2000, loss: 58.296288, accuracy: 0.925000\n",
      "step: 2100, loss: 64.351395, accuracy: 0.920000\n",
      "step: 2200, loss: 50.064522, accuracy: 0.915000\n",
      "step: 2300, loss: 65.303253, accuracy: 0.870000\n",
      "step: 2400, loss: 48.922577, accuracy: 0.915000\n",
      "step: 2500, loss: 56.494686, accuracy: 0.890000\n",
      "step: 2600, loss: 45.237724, accuracy: 0.915000\n",
      "step: 2700, loss: 53.050453, accuracy: 0.915000\n",
      "step: 2800, loss: 36.973473, accuracy: 0.945000\n",
      "step: 2900, loss: 46.336700, accuracy: 0.925000\n",
      "step: 3000, loss: 35.877079, accuracy: 0.945000\n",
      "step: 3100, loss: 40.618622, accuracy: 0.955000\n",
      "step: 3200, loss: 46.436783, accuracy: 0.925000\n",
      "step: 3300, loss: 32.660892, accuracy: 0.950000\n",
      "step: 3400, loss: 48.690048, accuracy: 0.915000\n",
      "step: 3500, loss: 48.585007, accuracy: 0.925000\n",
      "step: 3600, loss: 21.829857, accuracy: 0.970000\n",
      "step: 3700, loss: 36.676857, accuracy: 0.935000\n",
      "step: 3800, loss: 32.272804, accuracy: 0.955000\n",
      "step: 3900, loss: 14.635602, accuracy: 0.990000\n",
      "step: 4000, loss: 17.834610, accuracy: 0.975000\n",
      "step: 4100, loss: 35.287247, accuracy: 0.930000\n",
      "step: 4200, loss: 24.673740, accuracy: 0.965000\n",
      "step: 4300, loss: 17.337040, accuracy: 0.980000\n",
      "step: 4400, loss: 22.235201, accuracy: 0.960000\n",
      "step: 4500, loss: 35.036289, accuracy: 0.955000\n",
      "step: 4600, loss: 27.487545, accuracy: 0.950000\n",
      "step: 4700, loss: 18.605379, accuracy: 0.965000\n",
      "step: 4800, loss: 19.293571, accuracy: 0.970000\n",
      "step: 4900, loss: 26.759270, accuracy: 0.950000\n",
      "step: 5000, loss: 15.660883, accuracy: 0.975000\n",
      "step: 5100, loss: 18.702469, accuracy: 0.975000\n",
      "step: 5200, loss: 28.010897, accuracy: 0.950000\n",
      "step: 5300, loss: 19.765865, accuracy: 0.970000\n",
      "step: 5400, loss: 14.059657, accuracy: 0.980000\n",
      "step: 5500, loss: 17.831444, accuracy: 0.975000\n",
      "step: 5600, loss: 29.057898, accuracy: 0.955000\n",
      "step: 5700, loss: 15.531504, accuracy: 0.980000\n",
      "step: 5800, loss: 19.689281, accuracy: 0.970000\n",
      "step: 5900, loss: 25.739836, accuracy: 0.960000\n",
      "step: 6000, loss: 18.140835, accuracy: 0.980000\n",
      "step: 6100, loss: 21.343821, accuracy: 0.970000\n",
      "step: 6200, loss: 13.034136, accuracy: 0.985000\n",
      "step: 6300, loss: 24.486929, accuracy: 0.970000\n",
      "step: 6400, loss: 11.874432, accuracy: 0.980000\n",
      "step: 6500, loss: 12.749375, accuracy: 0.985000\n",
      "step: 6600, loss: 6.784030, accuracy: 0.995000\n",
      "step: 6700, loss: 17.610043, accuracy: 0.970000\n",
      "step: 6800, loss: 10.931812, accuracy: 0.975000\n",
      "step: 6900, loss: 21.918468, accuracy: 0.965000\n",
      "step: 7000, loss: 14.913073, accuracy: 0.970000\n",
      "step: 7100, loss: 22.545126, accuracy: 0.975000\n",
      "step: 7200, loss: 10.327265, accuracy: 0.990000\n",
      "step: 7300, loss: 16.031067, accuracy: 0.975000\n",
      "step: 7400, loss: 13.299742, accuracy: 0.985000\n",
      "step: 7500, loss: 12.504608, accuracy: 0.980000\n",
      "step: 7600, loss: 9.563753, accuracy: 0.990000\n",
      "step: 7700, loss: 14.674313, accuracy: 0.985000\n",
      "step: 7800, loss: 18.140472, accuracy: 0.980000\n",
      "step: 7900, loss: 5.922949, accuracy: 0.995000\n",
      "step: 8000, loss: 11.475697, accuracy: 0.990000\n",
      "step: 8100, loss: 10.119522, accuracy: 0.990000\n",
      "step: 8200, loss: 9.864338, accuracy: 0.990000\n",
      "step: 8300, loss: 11.662219, accuracy: 0.990000\n",
      "step: 8400, loss: 9.190744, accuracy: 0.985000\n",
      "step: 8500, loss: 9.922179, accuracy: 0.985000\n",
      "step: 8600, loss: 15.975153, accuracy: 0.980000\n",
      "step: 8700, loss: 9.668164, accuracy: 0.990000\n",
      "step: 8800, loss: 6.607253, accuracy: 0.990000\n",
      "step: 8900, loss: 7.831670, accuracy: 0.990000\n",
      "step: 9000, loss: 5.338233, accuracy: 1.000000\n",
      "step: 9100, loss: 3.240642, accuracy: 0.995000\n",
      "step: 9200, loss: 6.912164, accuracy: 0.985000\n",
      "step: 9300, loss: 8.518511, accuracy: 0.980000\n",
      "step: 9400, loss: 6.694562, accuracy: 0.990000\n",
      "step: 9500, loss: 7.185356, accuracy: 0.990000\n",
      "step: 9600, loss: 14.862761, accuracy: 0.985000\n",
      "step: 9700, loss: 6.408191, accuracy: 0.990000\n",
      "step: 9800, loss: 6.844561, accuracy: 0.990000\n",
      "step: 9900, loss: 6.266016, accuracy: 0.995000\n",
      "step: 10000, loss: 4.878545, accuracy: 0.995000\n",
      "step: 10100, loss: 5.794272, accuracy: 0.990000\n",
      "step: 10200, loss: 3.401376, accuracy: 1.000000\n",
      "step: 10300, loss: 3.753310, accuracy: 1.000000\n",
      "step: 10400, loss: 5.817682, accuracy: 0.995000\n",
      "step: 10500, loss: 5.307140, accuracy: 0.995000\n",
      "step: 10600, loss: 4.249178, accuracy: 1.000000\n",
      "step: 10700, loss: 9.349131, accuracy: 0.990000\n",
      "step: 10800, loss: 4.931638, accuracy: 0.985000\n",
      "step: 10900, loss: 3.057040, accuracy: 1.000000\n",
      "step: 11000, loss: 2.225171, accuracy: 1.000000\n",
      "step: 11100, loss: 3.339702, accuracy: 0.995000\n",
      "step: 11200, loss: 3.338702, accuracy: 0.995000\n",
      "step: 11300, loss: 3.929919, accuracy: 0.995000\n",
      "step: 11400, loss: 1.941365, accuracy: 1.000000\n",
      "step: 11500, loss: 3.501205, accuracy: 1.000000\n",
      "step: 11600, loss: 1.683769, accuracy: 1.000000\n",
      "step: 11700, loss: 3.744348, accuracy: 0.995000\n",
      "step: 11800, loss: 1.535930, accuracy: 1.000000\n",
      "step: 11900, loss: 1.992729, accuracy: 1.000000\n",
      "step: 12000, loss: 3.110128, accuracy: 0.995000\n",
      "step: 12100, loss: 1.649693, accuracy: 1.000000\n",
      "step: 12200, loss: 3.634145, accuracy: 1.000000\n",
      "step: 12300, loss: 3.771002, accuracy: 0.995000\n",
      "step: 12400, loss: 1.108886, accuracy: 1.000000\n",
      "step: 12500, loss: 2.709586, accuracy: 1.000000\n",
      "step: 12600, loss: 0.906489, accuracy: 1.000000\n",
      "step: 12700, loss: 0.961793, accuracy: 1.000000\n",
      "step: 12800, loss: 3.075524, accuracy: 1.000000\n",
      "step: 12900, loss: 1.201835, accuracy: 1.000000\n",
      "step: 13000, loss: 1.404070, accuracy: 1.000000\n",
      "step: 13100, loss: 1.020063, accuracy: 1.000000\n",
      "step: 13200, loss: 1.338537, accuracy: 1.000000\n",
      "step: 13300, loss: 1.115418, accuracy: 1.000000\n",
      "step: 13400, loss: 0.936788, accuracy: 1.000000\n",
      "step: 13500, loss: 1.660843, accuracy: 1.000000\n",
      "step: 13600, loss: 0.863946, accuracy: 1.000000\n",
      "step: 13700, loss: 1.548834, accuracy: 1.000000\n",
      "step: 13800, loss: 0.968002, accuracy: 1.000000\n",
      "step: 13900, loss: 0.789551, accuracy: 1.000000\n",
      "step: 14000, loss: 1.041989, accuracy: 1.000000\n",
      "step: 14100, loss: 1.235535, accuracy: 1.000000\n",
      "step: 14200, loss: 1.616689, accuracy: 1.000000\n",
      "step: 14300, loss: 0.805337, accuracy: 1.000000\n",
      "step: 14400, loss: 0.538122, accuracy: 1.000000\n",
      "step: 14500, loss: 1.361038, accuracy: 1.000000\n",
      "step: 14600, loss: 0.793839, accuracy: 1.000000\n",
      "step: 14700, loss: 0.804886, accuracy: 1.000000\n",
      "step: 14800, loss: 0.411648, accuracy: 1.000000\n",
      "step: 14900, loss: 0.555390, accuracy: 1.000000\n",
      "step: 15000, loss: 0.889160, accuracy: 1.000000\n",
      "step: 15100, loss: 0.349400, accuracy: 1.000000\n",
      "step: 15200, loss: 0.414475, accuracy: 1.000000\n",
      "step: 15300, loss: 0.323305, accuracy: 1.000000\n",
      "step: 15400, loss: 0.369228, accuracy: 1.000000\n",
      "step: 15500, loss: 0.522456, accuracy: 1.000000\n",
      "step: 15600, loss: 0.334338, accuracy: 1.000000\n",
      "step: 15700, loss: 0.322608, accuracy: 1.000000\n",
      "step: 15800, loss: 0.412158, accuracy: 1.000000\n",
      "step: 15900, loss: 0.490039, accuracy: 1.000000\n",
      "step: 16000, loss: 0.300518, accuracy: 1.000000\n",
      "step: 16100, loss: 0.398092, accuracy: 1.000000\n",
      "step: 16200, loss: 0.296122, accuracy: 1.000000\n",
      "step: 16300, loss: 0.200383, accuracy: 1.000000\n",
      "step: 16400, loss: 0.363882, accuracy: 1.000000\n",
      "step: 16500, loss: 0.217156, accuracy: 1.000000\n",
      "step: 16600, loss: 0.164320, accuracy: 1.000000\n",
      "step: 16700, loss: 0.195907, accuracy: 1.000000\n",
      "step: 16800, loss: 0.188612, accuracy: 1.000000\n",
      "step: 16900, loss: 0.230349, accuracy: 1.000000\n",
      "step: 17000, loss: 0.168376, accuracy: 1.000000\n",
      "step: 17100, loss: 0.117968, accuracy: 1.000000\n",
      "step: 17200, loss: 0.169680, accuracy: 1.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 17300, loss: 0.136138, accuracy: 1.000000\n",
      "step: 17400, loss: 0.130158, accuracy: 1.000000\n",
      "step: 17500, loss: 0.152585, accuracy: 1.000000\n",
      "step: 17600, loss: 0.149187, accuracy: 1.000000\n",
      "step: 17700, loss: 0.135753, accuracy: 1.000000\n",
      "step: 17800, loss: 0.108648, accuracy: 1.000000\n",
      "step: 17900, loss: 0.162171, accuracy: 1.000000\n",
      "step: 18000, loss: 0.081333, accuracy: 1.000000\n",
      "step: 18100, loss: 0.157047, accuracy: 1.000000\n",
      "step: 18200, loss: 0.178219, accuracy: 1.000000\n",
      "step: 18300, loss: 0.057791, accuracy: 1.000000\n",
      "step: 18400, loss: 0.130858, accuracy: 1.000000\n",
      "step: 18500, loss: 0.083979, accuracy: 1.000000\n",
      "step: 18600, loss: 1.600543, accuracy: 1.000000\n",
      "step: 18700, loss: 3.055218, accuracy: 0.995000\n",
      "step: 18800, loss: 0.405992, accuracy: 1.000000\n",
      "step: 18900, loss: 0.235850, accuracy: 1.000000\n",
      "step: 19000, loss: 0.239112, accuracy: 1.000000\n",
      "step: 19100, loss: 0.214890, accuracy: 1.000000\n",
      "step: 19200, loss: 0.216780, accuracy: 1.000000\n",
      "step: 19300, loss: 0.092365, accuracy: 1.000000\n",
      "step: 19400, loss: 0.089046, accuracy: 1.000000\n",
      "step: 19500, loss: 0.060075, accuracy: 1.000000\n",
      "step: 19600, loss: 0.078007, accuracy: 1.000000\n",
      "step: 19700, loss: 0.108590, accuracy: 1.000000\n",
      "step: 19800, loss: 0.087964, accuracy: 1.000000\n",
      "step: 19900, loss: 0.149929, accuracy: 1.000000\n",
      "step: 20000, loss: 0.085587, accuracy: 1.000000\n",
      "step: 20100, loss: 0.045572, accuracy: 1.000000\n",
      "step: 20200, loss: 0.072047, accuracy: 1.000000\n",
      "step: 20300, loss: 0.081578, accuracy: 1.000000\n",
      "step: 20400, loss: 0.054968, accuracy: 1.000000\n",
      "step: 20500, loss: 0.092469, accuracy: 1.000000\n",
      "step: 20600, loss: 0.059034, accuracy: 1.000000\n",
      "step: 20700, loss: 0.047460, accuracy: 1.000000\n",
      "step: 20800, loss: 0.097955, accuracy: 1.000000\n",
      "step: 20900, loss: 0.082450, accuracy: 1.000000\n",
      "step: 21000, loss: 0.056673, accuracy: 1.000000\n",
      "step: 21100, loss: 0.046401, accuracy: 1.000000\n",
      "step: 21200, loss: 0.044543, accuracy: 1.000000\n",
      "step: 21300, loss: 0.019819, accuracy: 1.000000\n",
      "step: 21400, loss: 0.057260, accuracy: 1.000000\n",
      "step: 21500, loss: 0.057062, accuracy: 1.000000\n",
      "step: 21600, loss: 0.047502, accuracy: 1.000000\n",
      "step: 21700, loss: 0.034927, accuracy: 1.000000\n",
      "step: 21800, loss: 0.056753, accuracy: 1.000000\n",
      "step: 21900, loss: 0.039702, accuracy: 1.000000\n",
      "step: 22000, loss: 0.052091, accuracy: 1.000000\n",
      "step: 22100, loss: 0.022152, accuracy: 1.000000\n",
      "step: 22200, loss: 0.045995, accuracy: 1.000000\n",
      "step: 22300, loss: 0.034465, accuracy: 1.000000\n",
      "step: 22400, loss: 0.050565, accuracy: 1.000000\n",
      "step: 22500, loss: 0.038236, accuracy: 1.000000\n",
      "step: 22600, loss: 0.037944, accuracy: 1.000000\n",
      "step: 22700, loss: 0.047485, accuracy: 1.000000\n",
      "step: 22800, loss: 0.045943, accuracy: 1.000000\n",
      "step: 22900, loss: 0.035361, accuracy: 1.000000\n",
      "step: 23000, loss: 0.035469, accuracy: 1.000000\n",
      "step: 23100, loss: 0.027885, accuracy: 1.000000\n",
      "step: 23200, loss: 0.037695, accuracy: 1.000000\n",
      "step: 23300, loss: 0.029935, accuracy: 1.000000\n",
      "step: 23400, loss: 0.023927, accuracy: 1.000000\n",
      "step: 23500, loss: 0.037605, accuracy: 1.000000\n",
      "step: 23600, loss: 0.039889, accuracy: 1.000000\n",
      "step: 23700, loss: 0.035364, accuracy: 1.000000\n",
      "step: 23800, loss: 0.021047, accuracy: 1.000000\n",
      "step: 23900, loss: 0.021843, accuracy: 1.000000\n",
      "step: 24000, loss: 0.026791, accuracy: 1.000000\n",
      "step: 24100, loss: 0.018094, accuracy: 1.000000\n",
      "step: 24200, loss: 0.036377, accuracy: 1.000000\n",
      "step: 24300, loss: 0.021484, accuracy: 1.000000\n",
      "step: 24400, loss: 0.028347, accuracy: 1.000000\n",
      "step: 24500, loss: 0.013829, accuracy: 1.000000\n",
      "step: 24600, loss: 0.010889, accuracy: 1.000000\n",
      "step: 24700, loss: 0.017442, accuracy: 1.000000\n",
      "step: 24800, loss: 0.022857, accuracy: 1.000000\n",
      "step: 24900, loss: 0.015218, accuracy: 1.000000\n",
      "step: 25000, loss: 0.014825, accuracy: 1.000000\n",
      "step: 25100, loss: 0.020338, accuracy: 1.000000\n",
      "step: 25200, loss: 0.014474, accuracy: 1.000000\n",
      "step: 25300, loss: 0.018764, accuracy: 1.000000\n",
      "step: 25400, loss: 0.014239, accuracy: 1.000000\n",
      "step: 25500, loss: 0.008622, accuracy: 1.000000\n",
      "step: 25600, loss: 0.012954, accuracy: 1.000000\n",
      "step: 25700, loss: 0.009992, accuracy: 1.000000\n",
      "step: 25800, loss: 0.009226, accuracy: 1.000000\n",
      "step: 25900, loss: 0.011078, accuracy: 1.000000\n",
      "step: 26000, loss: 0.010191, accuracy: 1.000000\n",
      "step: 26100, loss: 0.008448, accuracy: 1.000000\n",
      "step: 26200, loss: 0.010713, accuracy: 1.000000\n",
      "step: 26300, loss: 0.005248, accuracy: 1.000000\n",
      "step: 26400, loss: 0.012655, accuracy: 1.000000\n",
      "step: 26500, loss: 0.006218, accuracy: 1.000000\n",
      "step: 26600, loss: 0.009579, accuracy: 1.000000\n",
      "step: 26700, loss: 0.006109, accuracy: 1.000000\n",
      "step: 26800, loss: 0.008625, accuracy: 1.000000\n",
      "step: 26900, loss: 0.005538, accuracy: 1.000000\n",
      "step: 27000, loss: 0.004654, accuracy: 1.000000\n",
      "step: 27100, loss: 0.006821, accuracy: 1.000000\n",
      "step: 27200, loss: 0.004604, accuracy: 1.000000\n",
      "step: 27300, loss: 0.004811, accuracy: 1.000000\n",
      "step: 27400, loss: 0.006899, accuracy: 1.000000\n",
      "step: 27500, loss: 0.004295, accuracy: 1.000000\n",
      "step: 27600, loss: 0.003457, accuracy: 1.000000\n",
      "step: 27700, loss: 0.003893, accuracy: 1.000000\n",
      "step: 27800, loss: 0.006378, accuracy: 1.000000\n",
      "step: 27900, loss: 0.002972, accuracy: 1.000000\n",
      "step: 28000, loss: 0.003746, accuracy: 1.000000\n",
      "step: 28100, loss: 0.003095, accuracy: 1.000000\n",
      "step: 28200, loss: 0.004468, accuracy: 1.000000\n",
      "step: 28300, loss: 0.003011, accuracy: 1.000000\n",
      "step: 28400, loss: 0.002561, accuracy: 1.000000\n",
      "step: 28500, loss: 0.003785, accuracy: 1.000000\n",
      "step: 28600, loss: 0.002808, accuracy: 1.000000\n",
      "step: 28700, loss: 0.002959, accuracy: 1.000000\n",
      "step: 28800, loss: 0.001982, accuracy: 1.000000\n",
      "step: 28900, loss: 0.002481, accuracy: 1.000000\n",
      "step: 29000, loss: 0.002299, accuracy: 1.000000\n",
      "step: 29100, loss: 0.001571, accuracy: 1.000000\n",
      "step: 29200, loss: 0.002488, accuracy: 1.000000\n",
      "step: 29300, loss: 0.001993, accuracy: 1.000000\n",
      "step: 29400, loss: 0.001847, accuracy: 1.000000\n",
      "step: 29500, loss: 0.000961, accuracy: 1.000000\n",
      "step: 29600, loss: 0.001247, accuracy: 1.000000\n",
      "step: 29700, loss: 0.001375, accuracy: 1.000000\n",
      "step: 29800, loss: 0.001652, accuracy: 1.000000\n",
      "step: 29900, loss: 0.001634, accuracy: 1.000000\n",
      "step: 30000, loss: 0.000855, accuracy: 1.000000\n",
      "step: 30100, loss: 0.001199, accuracy: 1.000000\n",
      "step: 30200, loss: 0.001324, accuracy: 1.000000\n",
      "step: 30300, loss: 0.001219, accuracy: 1.000000\n",
      "step: 30400, loss: 1.658049, accuracy: 0.995000\n",
      "step: 30500, loss: 0.151540, accuracy: 1.000000\n",
      "step: 30600, loss: 0.369031, accuracy: 1.000000\n",
      "step: 30700, loss: 0.126721, accuracy: 1.000000\n",
      "step: 30800, loss: 0.132133, accuracy: 1.000000\n",
      "step: 30900, loss: 0.012768, accuracy: 1.000000\n",
      "step: 31000, loss: 0.020102, accuracy: 1.000000\n",
      "step: 31100, loss: 0.023245, accuracy: 1.000000\n",
      "step: 31200, loss: 0.013492, accuracy: 1.000000\n",
      "step: 31300, loss: 0.023701, accuracy: 1.000000\n",
      "step: 31400, loss: 0.010694, accuracy: 1.000000\n",
      "step: 31500, loss: 0.023865, accuracy: 1.000000\n",
      "step: 31600, loss: 0.017537, accuracy: 1.000000\n",
      "step: 31700, loss: 0.007668, accuracy: 1.000000\n",
      "step: 31800, loss: 0.015646, accuracy: 1.000000\n",
      "step: 31900, loss: 0.011231, accuracy: 1.000000\n",
      "step: 32000, loss: 0.017187, accuracy: 1.000000\n",
      "step: 32100, loss: 0.006811, accuracy: 1.000000\n",
      "step: 32200, loss: 0.013387, accuracy: 1.000000\n",
      "step: 32300, loss: 0.006987, accuracy: 1.000000\n",
      "step: 32400, loss: 0.012878, accuracy: 1.000000\n",
      "step: 32500, loss: 0.016162, accuracy: 1.000000\n",
      "step: 32600, loss: 0.010756, accuracy: 1.000000\n",
      "step: 32700, loss: 0.005199, accuracy: 1.000000\n",
      "step: 32800, loss: 0.012483, accuracy: 1.000000\n",
      "step: 32900, loss: 0.017518, accuracy: 1.000000\n",
      "step: 33000, loss: 0.015237, accuracy: 1.000000\n",
      "step: 33100, loss: 0.011187, accuracy: 1.000000\n",
      "step: 33200, loss: 0.008531, accuracy: 1.000000\n",
      "step: 33300, loss: 0.004668, accuracy: 1.000000\n",
      "step: 33400, loss: 0.012154, accuracy: 1.000000\n",
      "step: 33500, loss: 0.016897, accuracy: 1.000000\n",
      "step: 33600, loss: 0.009468, accuracy: 1.000000\n",
      "step: 33700, loss: 0.006442, accuracy: 1.000000\n",
      "step: 33800, loss: 0.012531, accuracy: 1.000000\n",
      "step: 33900, loss: 0.006990, accuracy: 1.000000\n",
      "step: 34000, loss: 0.005659, accuracy: 1.000000\n",
      "step: 34100, loss: 0.009338, accuracy: 1.000000\n",
      "step: 34200, loss: 0.007283, accuracy: 1.000000\n",
      "step: 34300, loss: 0.008661, accuracy: 1.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 34400, loss: 0.005554, accuracy: 1.000000\n",
      "step: 34500, loss: 0.002637, accuracy: 1.000000\n",
      "step: 34600, loss: 0.005770, accuracy: 1.000000\n",
      "step: 34700, loss: 0.006139, accuracy: 1.000000\n",
      "step: 34800, loss: 0.005103, accuracy: 1.000000\n",
      "step: 34900, loss: 0.004859, accuracy: 1.000000\n",
      "step: 35000, loss: 0.007041, accuracy: 1.000000\n",
      "step: 35100, loss: 0.004099, accuracy: 1.000000\n",
      "step: 35200, loss: 0.005876, accuracy: 1.000000\n",
      "step: 35300, loss: 0.005577, accuracy: 1.000000\n",
      "step: 35400, loss: 0.006302, accuracy: 1.000000\n",
      "step: 35500, loss: 0.003362, accuracy: 1.000000\n",
      "step: 35600, loss: 0.003457, accuracy: 1.000000\n",
      "step: 35700, loss: 0.003887, accuracy: 1.000000\n",
      "step: 35800, loss: 0.002949, accuracy: 1.000000\n",
      "step: 35900, loss: 0.004871, accuracy: 1.000000\n",
      "step: 36000, loss: 0.001544, accuracy: 1.000000\n",
      "step: 36100, loss: 0.005253, accuracy: 1.000000\n",
      "step: 36200, loss: 0.004251, accuracy: 1.000000\n",
      "step: 36300, loss: 0.002402, accuracy: 1.000000\n",
      "step: 36400, loss: 0.003056, accuracy: 1.000000\n",
      "step: 36500, loss: 0.003164, accuracy: 1.000000\n",
      "step: 36600, loss: 0.001339, accuracy: 1.000000\n",
      "step: 36700, loss: 0.001486, accuracy: 1.000000\n",
      "step: 36800, loss: 0.004579, accuracy: 1.000000\n",
      "step: 36900, loss: 0.002828, accuracy: 1.000000\n",
      "step: 37000, loss: 0.002277, accuracy: 1.000000\n",
      "step: 37100, loss: 0.003607, accuracy: 1.000000\n",
      "step: 37200, loss: 0.002782, accuracy: 1.000000\n",
      "step: 37300, loss: 0.001584, accuracy: 1.000000\n",
      "step: 37400, loss: 0.002421, accuracy: 1.000000\n",
      "step: 37500, loss: 0.002044, accuracy: 1.000000\n",
      "step: 37600, loss: 0.002106, accuracy: 1.000000\n",
      "step: 37700, loss: 0.002899, accuracy: 1.000000\n",
      "step: 37800, loss: 0.001346, accuracy: 1.000000\n",
      "step: 37900, loss: 0.001867, accuracy: 1.000000\n",
      "step: 38000, loss: 0.001967, accuracy: 1.000000\n",
      "step: 38100, loss: 0.002045, accuracy: 1.000000\n",
      "step: 38200, loss: 0.000997, accuracy: 1.000000\n",
      "step: 38300, loss: 0.002781, accuracy: 1.000000\n",
      "step: 38400, loss: 0.001383, accuracy: 1.000000\n",
      "step: 38500, loss: 0.001290, accuracy: 1.000000\n",
      "step: 38600, loss: 0.002400, accuracy: 1.000000\n",
      "step: 38700, loss: 0.001364, accuracy: 1.000000\n",
      "step: 38800, loss: 0.001142, accuracy: 1.000000\n",
      "step: 38900, loss: 0.001555, accuracy: 1.000000\n",
      "step: 39000, loss: 0.001054, accuracy: 1.000000\n",
      "step: 39100, loss: 0.001926, accuracy: 1.000000\n",
      "step: 39200, loss: 0.001055, accuracy: 1.000000\n",
      "step: 39300, loss: 0.001978, accuracy: 1.000000\n",
      "step: 39400, loss: 0.001233, accuracy: 1.000000\n",
      "step: 39500, loss: 0.002178, accuracy: 1.000000\n",
      "step: 39600, loss: 0.000960, accuracy: 1.000000\n",
      "step: 39700, loss: 0.001722, accuracy: 1.000000\n",
      "step: 39800, loss: 0.001344, accuracy: 1.000000\n",
      "step: 39900, loss: 0.001992, accuracy: 1.000000\n",
      "step: 40000, loss: 0.001263, accuracy: 1.000000\n",
      "step: 40100, loss: 0.001274, accuracy: 1.000000\n",
      "step: 40200, loss: 0.001105, accuracy: 1.000000\n",
      "step: 40300, loss: 0.000649, accuracy: 1.000000\n",
      "step: 40400, loss: 0.000681, accuracy: 1.000000\n",
      "step: 40500, loss: 0.000336, accuracy: 1.000000\n",
      "step: 40600, loss: 0.001056, accuracy: 1.000000\n",
      "step: 40700, loss: 0.000835, accuracy: 1.000000\n",
      "step: 40800, loss: 0.000846, accuracy: 1.000000\n",
      "step: 40900, loss: 0.000544, accuracy: 1.000000\n",
      "step: 41000, loss: 0.000814, accuracy: 1.000000\n",
      "step: 41100, loss: 0.000371, accuracy: 1.000000\n",
      "step: 41200, loss: 0.000645, accuracy: 1.000000\n",
      "step: 41300, loss: 0.000719, accuracy: 1.000000\n",
      "step: 41400, loss: 0.000682, accuracy: 1.000000\n",
      "step: 41500, loss: 0.000734, accuracy: 1.000000\n",
      "step: 41600, loss: 0.000731, accuracy: 1.000000\n",
      "step: 41700, loss: 0.000529, accuracy: 1.000000\n",
      "step: 41800, loss: 0.000577, accuracy: 1.000000\n",
      "step: 41900, loss: 0.000455, accuracy: 1.000000\n",
      "step: 42000, loss: 0.000605, accuracy: 1.000000\n",
      "step: 42100, loss: 0.000471, accuracy: 1.000000\n",
      "step: 42200, loss: 0.000368, accuracy: 1.000000\n",
      "step: 42300, loss: 0.000399, accuracy: 1.000000\n",
      "step: 42400, loss: 0.000386, accuracy: 1.000000\n",
      "step: 42500, loss: 0.000807, accuracy: 1.000000\n",
      "step: 42600, loss: 0.000457, accuracy: 1.000000\n",
      "step: 42700, loss: 0.000572, accuracy: 1.000000\n",
      "step: 42800, loss: 0.000416, accuracy: 1.000000\n",
      "step: 42900, loss: 0.000358, accuracy: 1.000000\n",
      "step: 43000, loss: 0.000461, accuracy: 1.000000\n",
      "step: 43100, loss: 0.000414, accuracy: 1.000000\n",
      "step: 43200, loss: 0.000443, accuracy: 1.000000\n",
      "step: 43300, loss: 0.000378, accuracy: 1.000000\n",
      "step: 43400, loss: 0.000319, accuracy: 1.000000\n",
      "step: 43500, loss: 0.000363, accuracy: 1.000000\n",
      "step: 43600, loss: 0.000303, accuracy: 1.000000\n",
      "step: 43700, loss: 0.000158, accuracy: 1.000000\n",
      "step: 43800, loss: 0.000269, accuracy: 1.000000\n",
      "step: 43900, loss: 0.000279, accuracy: 1.000000\n",
      "step: 44000, loss: 0.000188, accuracy: 1.000000\n",
      "step: 44100, loss: 0.000227, accuracy: 1.000000\n",
      "step: 44200, loss: 0.000230, accuracy: 1.000000\n",
      "step: 44300, loss: 0.000212, accuracy: 1.000000\n",
      "step: 44400, loss: 0.000122, accuracy: 1.000000\n",
      "step: 44500, loss: 0.000200, accuracy: 1.000000\n",
      "step: 44600, loss: 0.000239, accuracy: 1.000000\n",
      "step: 44700, loss: 0.000115, accuracy: 1.000000\n",
      "step: 44800, loss: 0.000200, accuracy: 1.000000\n",
      "step: 44900, loss: 0.000134, accuracy: 1.000000\n",
      "step: 45000, loss: 0.000080, accuracy: 1.000000\n",
      "step: 45100, loss: 0.000118, accuracy: 1.000000\n",
      "step: 45200, loss: 0.000082, accuracy: 1.000000\n",
      "step: 45300, loss: 0.000138, accuracy: 1.000000\n",
      "step: 45400, loss: 0.000150, accuracy: 1.000000\n",
      "step: 45500, loss: 0.000168, accuracy: 1.000000\n",
      "step: 45600, loss: 0.000069, accuracy: 1.000000\n",
      "step: 45700, loss: 0.000088, accuracy: 1.000000\n",
      "step: 45800, loss: 0.000100, accuracy: 1.000000\n",
      "step: 45900, loss: 0.000108, accuracy: 1.000000\n",
      "step: 46000, loss: 0.000143, accuracy: 1.000000\n",
      "step: 46100, loss: 0.000069, accuracy: 1.000000\n",
      "step: 46200, loss: 0.000072, accuracy: 1.000000\n",
      "step: 46300, loss: 0.000079, accuracy: 1.000000\n",
      "step: 46400, loss: 0.000064, accuracy: 1.000000\n",
      "step: 46500, loss: 0.000070, accuracy: 1.000000\n",
      "step: 46600, loss: 0.000069, accuracy: 1.000000\n",
      "step: 46700, loss: 0.000045, accuracy: 1.000000\n",
      "step: 46800, loss: 0.000058, accuracy: 1.000000\n",
      "step: 46900, loss: 0.000073, accuracy: 1.000000\n",
      "step: 47000, loss: 0.000080, accuracy: 1.000000\n",
      "step: 47100, loss: 0.000027, accuracy: 1.000000\n",
      "step: 47200, loss: 0.000063, accuracy: 1.000000\n",
      "step: 47300, loss: 0.000066, accuracy: 1.000000\n",
      "step: 47400, loss: 0.000049, accuracy: 1.000000\n",
      "step: 47500, loss: 0.000039, accuracy: 1.000000\n",
      "step: 47600, loss: 0.000045, accuracy: 1.000000\n",
      "step: 47700, loss: 0.000042, accuracy: 1.000000\n",
      "step: 47800, loss: 0.000035, accuracy: 1.000000\n",
      "step: 47900, loss: 0.000028, accuracy: 1.000000\n",
      "step: 48000, loss: 0.000041, accuracy: 1.000000\n",
      "step: 48100, loss: 0.000027, accuracy: 1.000000\n",
      "step: 48200, loss: 0.000037, accuracy: 1.000000\n",
      "step: 48300, loss: 0.000026, accuracy: 1.000000\n",
      "step: 48400, loss: 0.000017, accuracy: 1.000000\n",
      "step: 48500, loss: 0.000033, accuracy: 1.000000\n",
      "step: 48600, loss: 0.000026, accuracy: 1.000000\n",
      "step: 48700, loss: 0.000026, accuracy: 1.000000\n",
      "step: 48800, loss: 0.000020, accuracy: 1.000000\n",
      "step: 48900, loss: 0.000030, accuracy: 1.000000\n",
      "step: 49000, loss: 0.000011, accuracy: 1.000000\n",
      "step: 49100, loss: 0.000022, accuracy: 1.000000\n",
      "step: 49200, loss: 0.000014, accuracy: 1.000000\n",
      "step: 49300, loss: 0.000013, accuracy: 1.000000\n",
      "step: 49400, loss: 0.000010, accuracy: 1.000000\n",
      "step: 49500, loss: 0.000018, accuracy: 1.000000\n",
      "step: 49600, loss: 0.000016, accuracy: 1.000000\n",
      "step: 49700, loss: 0.000017, accuracy: 1.000000\n",
      "step: 49800, loss: 0.000012, accuracy: 1.000000\n",
      "step: 49900, loss: 0.000022, accuracy: 1.000000\n",
      "step: 50000, loss: 0.000017, accuracy: 1.000000\n",
      "step: 50100, loss: 0.000009, accuracy: 1.000000\n",
      "step: 50200, loss: 0.000012, accuracy: 1.000000\n",
      "step: 50300, loss: 0.000013, accuracy: 1.000000\n",
      "step: 50400, loss: 0.000013, accuracy: 1.000000\n",
      "step: 50500, loss: 0.000008, accuracy: 1.000000\n",
      "step: 50600, loss: 0.000006, accuracy: 1.000000\n",
      "step: 50700, loss: 0.000010, accuracy: 1.000000\n",
      "step: 50800, loss: 0.000005, accuracy: 1.000000\n",
      "step: 50900, loss: 0.000009, accuracy: 1.000000\n",
      "step: 51000, loss: 0.000008, accuracy: 1.000000\n",
      "step: 51100, loss: 0.000007, accuracy: 1.000000\n",
      "step: 51200, loss: 0.000006, accuracy: 1.000000\n",
      "step: 51300, loss: 0.000004, accuracy: 1.000000\n",
      "step: 51400, loss: 0.000005, accuracy: 1.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 51500, loss: 0.000005, accuracy: 1.000000\n",
      "step: 51600, loss: 0.000004, accuracy: 1.000000\n",
      "step: 51700, loss: 0.000006, accuracy: 1.000000\n",
      "step: 51800, loss: 0.000005, accuracy: 1.000000\n",
      "step: 51900, loss: 0.000006, accuracy: 1.000000\n",
      "step: 52000, loss: 0.000004, accuracy: 1.000000\n",
      "step: 52100, loss: 0.000005, accuracy: 1.000000\n",
      "step: 52200, loss: 0.000004, accuracy: 1.000000\n",
      "step: 52300, loss: 0.000003, accuracy: 1.000000\n",
      "step: 52400, loss: 0.000006, accuracy: 1.000000\n",
      "step: 52500, loss: 0.000005, accuracy: 1.000000\n",
      "step: 52600, loss: 0.000003, accuracy: 1.000000\n",
      "step: 52700, loss: 0.000004, accuracy: 1.000000\n",
      "step: 52800, loss: 0.000004, accuracy: 1.000000\n",
      "step: 52900, loss: 0.000003, accuracy: 1.000000\n",
      "step: 53000, loss: 0.000003, accuracy: 1.000000\n",
      "step: 53100, loss: 0.000002, accuracy: 1.000000\n",
      "step: 53200, loss: 0.000003, accuracy: 1.000000\n",
      "step: 53300, loss: 0.000002, accuracy: 1.000000\n",
      "step: 53400, loss: 0.000002, accuracy: 1.000000\n",
      "step: 53500, loss: 0.000003, accuracy: 1.000000\n",
      "step: 53600, loss: 0.000002, accuracy: 1.000000\n",
      "step: 53700, loss: 0.000002, accuracy: 1.000000\n",
      "step: 53800, loss: 0.000002, accuracy: 1.000000\n",
      "step: 53900, loss: 0.000002, accuracy: 1.000000\n",
      "step: 54000, loss: 0.000001, accuracy: 1.000000\n",
      "step: 54100, loss: 0.000001, accuracy: 1.000000\n",
      "step: 54200, loss: 0.000003, accuracy: 1.000000\n",
      "step: 54300, loss: 0.000003, accuracy: 1.000000\n",
      "step: 54400, loss: 0.000001, accuracy: 1.000000\n",
      "step: 54500, loss: 0.000001, accuracy: 1.000000\n",
      "step: 54600, loss: 0.000001, accuracy: 1.000000\n",
      "step: 54700, loss: 0.000001, accuracy: 1.000000\n",
      "step: 54800, loss: 0.000001, accuracy: 1.000000\n",
      "step: 54900, loss: 0.000001, accuracy: 1.000000\n",
      "step: 55000, loss: 0.000000, accuracy: 1.000000\n",
      "step: 55100, loss: 0.000002, accuracy: 1.000000\n",
      "step: 55200, loss: 0.000001, accuracy: 1.000000\n",
      "step: 55300, loss: 0.000001, accuracy: 1.000000\n",
      "step: 55400, loss: 0.000001, accuracy: 1.000000\n",
      "step: 55500, loss: 0.000001, accuracy: 1.000000\n",
      "step: 55600, loss: 0.000001, accuracy: 1.000000\n",
      "step: 55700, loss: 0.000000, accuracy: 1.000000\n",
      "step: 55800, loss: 0.000001, accuracy: 1.000000\n",
      "step: 55900, loss: 0.000002, accuracy: 1.000000\n",
      "step: 56000, loss: 0.000002, accuracy: 1.000000\n",
      "step: 56100, loss: 0.000000, accuracy: 1.000000\n",
      "step: 56200, loss: 0.000001, accuracy: 1.000000\n",
      "step: 56300, loss: 0.000001, accuracy: 1.000000\n",
      "step: 56400, loss: 0.000001, accuracy: 1.000000\n",
      "step: 56500, loss: 0.000001, accuracy: 1.000000\n",
      "step: 56600, loss: 0.000000, accuracy: 1.000000\n",
      "step: 56700, loss: 0.000000, accuracy: 1.000000\n",
      "step: 56800, loss: 0.000000, accuracy: 1.000000\n",
      "step: 56900, loss: 0.000002, accuracy: 1.000000\n",
      "step: 57000, loss: 0.000001, accuracy: 1.000000\n",
      "step: 57100, loss: 0.000001, accuracy: 1.000000\n",
      "step: 57200, loss: 0.058183, accuracy: 1.000000\n",
      "step: 57300, loss: 0.503778, accuracy: 1.000000\n",
      "step: 57400, loss: 0.015797, accuracy: 1.000000\n",
      "step: 57500, loss: 0.120823, accuracy: 1.000000\n",
      "step: 57600, loss: 0.009374, accuracy: 1.000000\n",
      "step: 57700, loss: 0.003358, accuracy: 1.000000\n",
      "step: 57800, loss: 0.003485, accuracy: 1.000000\n",
      "step: 57900, loss: 0.002448, accuracy: 1.000000\n",
      "step: 58000, loss: 0.006353, accuracy: 1.000000\n",
      "step: 58100, loss: 0.001983, accuracy: 1.000000\n",
      "step: 58200, loss: 0.027948, accuracy: 1.000000\n",
      "step: 58300, loss: 0.000389, accuracy: 1.000000\n",
      "step: 58400, loss: 0.000441, accuracy: 1.000000\n",
      "step: 58500, loss: 0.005840, accuracy: 1.000000\n",
      "step: 58600, loss: 0.000472, accuracy: 1.000000\n",
      "step: 58700, loss: 0.001365, accuracy: 1.000000\n",
      "step: 58800, loss: 0.000498, accuracy: 1.000000\n",
      "step: 58900, loss: 0.000242, accuracy: 1.000000\n",
      "step: 59000, loss: 0.001120, accuracy: 1.000000\n",
      "step: 59100, loss: 0.003291, accuracy: 1.000000\n",
      "step: 59200, loss: 0.004088, accuracy: 1.000000\n",
      "step: 59300, loss: 0.000655, accuracy: 1.000000\n",
      "step: 59400, loss: 0.000741, accuracy: 1.000000\n",
      "step: 59500, loss: 0.001317, accuracy: 1.000000\n",
      "step: 59600, loss: 0.001131, accuracy: 1.000000\n",
      "step: 59700, loss: 0.000135, accuracy: 1.000000\n",
      "step: 59800, loss: 0.000516, accuracy: 1.000000\n",
      "step: 59900, loss: 0.001664, accuracy: 1.000000\n",
      "step: 60000, loss: 0.000896, accuracy: 1.000000\n",
      "step: 60100, loss: 0.000276, accuracy: 1.000000\n",
      "step: 60200, loss: 0.000812, accuracy: 1.000000\n",
      "step: 60300, loss: 0.001004, accuracy: 1.000000\n",
      "step: 60400, loss: 0.000684, accuracy: 1.000000\n",
      "step: 60500, loss: 0.001164, accuracy: 1.000000\n",
      "step: 60600, loss: 0.000040, accuracy: 1.000000\n",
      "step: 60700, loss: 0.000686, accuracy: 1.000000\n",
      "step: 60800, loss: 0.001145, accuracy: 1.000000\n",
      "step: 60900, loss: 0.000952, accuracy: 1.000000\n",
      "step: 61000, loss: 0.002041, accuracy: 1.000000\n",
      "step: 61100, loss: 0.000491, accuracy: 1.000000\n",
      "step: 61200, loss: 0.000427, accuracy: 1.000000\n",
      "step: 61300, loss: 0.000553, accuracy: 1.000000\n",
      "step: 61400, loss: 0.000292, accuracy: 1.000000\n",
      "step: 61500, loss: 0.000015, accuracy: 1.000000\n",
      "step: 61600, loss: 0.000504, accuracy: 1.000000\n",
      "step: 61700, loss: 0.000776, accuracy: 1.000000\n",
      "step: 61800, loss: 0.000670, accuracy: 1.000000\n",
      "step: 61900, loss: 0.000652, accuracy: 1.000000\n",
      "step: 62000, loss: 0.000538, accuracy: 1.000000\n",
      "step: 62100, loss: 0.000384, accuracy: 1.000000\n",
      "step: 62200, loss: 0.001430, accuracy: 1.000000\n",
      "step: 62300, loss: 0.000521, accuracy: 1.000000\n",
      "step: 62400, loss: 0.000810, accuracy: 1.000000\n",
      "step: 62500, loss: 0.000029, accuracy: 1.000000\n",
      "step: 62600, loss: 0.000395, accuracy: 1.000000\n",
      "step: 62700, loss: 0.000128, accuracy: 1.000000\n",
      "step: 62800, loss: 0.001003, accuracy: 1.000000\n",
      "step: 62900, loss: 0.000260, accuracy: 1.000000\n",
      "step: 63000, loss: 0.000306, accuracy: 1.000000\n",
      "step: 63100, loss: 0.000702, accuracy: 1.000000\n",
      "step: 63200, loss: 0.000230, accuracy: 1.000000\n",
      "step: 63300, loss: 0.000169, accuracy: 1.000000\n",
      "step: 63400, loss: 0.000489, accuracy: 1.000000\n",
      "step: 63500, loss: 0.000294, accuracy: 1.000000\n",
      "step: 63600, loss: 0.000336, accuracy: 1.000000\n",
      "step: 63700, loss: 0.000509, accuracy: 1.000000\n",
      "step: 63800, loss: 0.000241, accuracy: 1.000000\n",
      "step: 63900, loss: 0.000197, accuracy: 1.000000\n",
      "step: 64000, loss: 0.000250, accuracy: 1.000000\n",
      "step: 64100, loss: 0.000114, accuracy: 1.000000\n",
      "step: 64200, loss: 0.000302, accuracy: 1.000000\n",
      "step: 64300, loss: 0.000237, accuracy: 1.000000\n",
      "step: 64400, loss: 0.000224, accuracy: 1.000000\n",
      "step: 64500, loss: 0.000221, accuracy: 1.000000\n",
      "step: 64600, loss: 0.000222, accuracy: 1.000000\n",
      "step: 64700, loss: 0.000230, accuracy: 1.000000\n",
      "step: 64800, loss: 0.000086, accuracy: 1.000000\n",
      "step: 64900, loss: 0.000058, accuracy: 1.000000\n",
      "step: 65000, loss: 0.000170, accuracy: 1.000000\n",
      "step: 65100, loss: 0.000140, accuracy: 1.000000\n",
      "step: 65200, loss: 0.000147, accuracy: 1.000000\n",
      "step: 65300, loss: 0.000060, accuracy: 1.000000\n",
      "step: 65400, loss: 0.000091, accuracy: 1.000000\n",
      "step: 65500, loss: 0.000149, accuracy: 1.000000\n",
      "step: 65600, loss: 0.000143, accuracy: 1.000000\n",
      "step: 65700, loss: 0.000035, accuracy: 1.000000\n",
      "step: 65800, loss: 0.000165, accuracy: 1.000000\n",
      "step: 65900, loss: 0.000107, accuracy: 1.000000\n",
      "step: 66000, loss: 0.000071, accuracy: 1.000000\n",
      "step: 66100, loss: 0.000059, accuracy: 1.000000\n",
      "step: 66200, loss: 0.000071, accuracy: 1.000000\n",
      "step: 66300, loss: 0.000051, accuracy: 1.000000\n",
      "step: 66400, loss: 0.000062, accuracy: 1.000000\n",
      "step: 66500, loss: 0.000106, accuracy: 1.000000\n",
      "step: 66600, loss: 0.000149, accuracy: 1.000000\n",
      "step: 66700, loss: 0.000150, accuracy: 1.000000\n",
      "step: 66800, loss: 0.000079, accuracy: 1.000000\n",
      "step: 66900, loss: 0.000127, accuracy: 1.000000\n",
      "step: 67000, loss: 0.000052, accuracy: 1.000000\n",
      "step: 67100, loss: 0.000128, accuracy: 1.000000\n",
      "step: 67200, loss: 0.000111, accuracy: 1.000000\n",
      "step: 67300, loss: 0.000171, accuracy: 1.000000\n",
      "step: 67400, loss: 0.000063, accuracy: 1.000000\n",
      "step: 67500, loss: 0.000055, accuracy: 1.000000\n",
      "step: 67600, loss: 0.000026, accuracy: 1.000000\n",
      "step: 67700, loss: 0.000092, accuracy: 1.000000\n",
      "step: 67800, loss: 0.000025, accuracy: 1.000000\n",
      "step: 67900, loss: 0.000030, accuracy: 1.000000\n",
      "step: 68000, loss: 0.000084, accuracy: 1.000000\n",
      "step: 68100, loss: 0.000048, accuracy: 1.000000\n",
      "step: 68200, loss: 0.000050, accuracy: 1.000000\n",
      "step: 68300, loss: 0.000084, accuracy: 1.000000\n",
      "step: 68400, loss: 0.000078, accuracy: 1.000000\n",
      "step: 68500, loss: 0.000037, accuracy: 1.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 68600, loss: 0.000058, accuracy: 1.000000\n",
      "step: 68700, loss: 0.000018, accuracy: 1.000000\n",
      "step: 68800, loss: 0.000029, accuracy: 1.000000\n",
      "step: 68900, loss: 0.000029, accuracy: 1.000000\n",
      "step: 69000, loss: 0.000013, accuracy: 1.000000\n",
      "step: 69100, loss: 0.000039, accuracy: 1.000000\n",
      "step: 69200, loss: 0.000023, accuracy: 1.000000\n",
      "step: 69300, loss: 0.000047, accuracy: 1.000000\n",
      "step: 69400, loss: 0.000041, accuracy: 1.000000\n",
      "step: 69500, loss: 0.000039, accuracy: 1.000000\n",
      "step: 69600, loss: 0.000009, accuracy: 1.000000\n",
      "step: 69700, loss: 0.000017, accuracy: 1.000000\n",
      "step: 69800, loss: 0.000024, accuracy: 1.000000\n",
      "step: 69900, loss: 0.000025, accuracy: 1.000000\n",
      "step: 70000, loss: 0.000015, accuracy: 1.000000\n",
      "step: 70100, loss: 0.000034, accuracy: 1.000000\n",
      "step: 70200, loss: 0.000036, accuracy: 1.000000\n",
      "step: 70300, loss: 0.000011, accuracy: 1.000000\n",
      "step: 70400, loss: 0.000032, accuracy: 1.000000\n",
      "step: 70500, loss: 0.000018, accuracy: 1.000000\n",
      "step: 70600, loss: 0.000022, accuracy: 1.000000\n",
      "step: 70700, loss: 0.000010, accuracy: 1.000000\n",
      "step: 70800, loss: 0.000010, accuracy: 1.000000\n",
      "step: 70900, loss: 0.000025, accuracy: 1.000000\n",
      "step: 71000, loss: 0.000027, accuracy: 1.000000\n",
      "step: 71100, loss: 0.000019, accuracy: 1.000000\n",
      "step: 71200, loss: 0.000017, accuracy: 1.000000\n",
      "step: 71300, loss: 0.000019, accuracy: 1.000000\n",
      "step: 71400, loss: 0.000013, accuracy: 1.000000\n",
      "step: 71500, loss: 0.000012, accuracy: 1.000000\n",
      "step: 71600, loss: 0.000017, accuracy: 1.000000\n",
      "step: 71700, loss: 0.000007, accuracy: 1.000000\n",
      "step: 71800, loss: 0.000022, accuracy: 1.000000\n",
      "step: 71900, loss: 0.000006, accuracy: 1.000000\n",
      "step: 72000, loss: 0.000012, accuracy: 1.000000\n",
      "step: 72100, loss: 0.000018, accuracy: 1.000000\n",
      "step: 72200, loss: 0.000010, accuracy: 1.000000\n",
      "step: 72300, loss: 0.000010, accuracy: 1.000000\n",
      "step: 72400, loss: 0.000012, accuracy: 1.000000\n",
      "step: 72500, loss: 0.000011, accuracy: 1.000000\n",
      "step: 72600, loss: 0.000010, accuracy: 1.000000\n",
      "step: 72700, loss: 0.000012, accuracy: 1.000000\n",
      "step: 72800, loss: 0.000011, accuracy: 1.000000\n",
      "step: 72900, loss: 0.000006, accuracy: 1.000000\n",
      "step: 73000, loss: 0.000003, accuracy: 1.000000\n",
      "step: 73100, loss: 0.000019, accuracy: 1.000000\n",
      "step: 73200, loss: 0.000003, accuracy: 1.000000\n",
      "step: 73300, loss: 0.000008, accuracy: 1.000000\n",
      "step: 73400, loss: 0.000011, accuracy: 1.000000\n",
      "step: 73500, loss: 0.000005, accuracy: 1.000000\n",
      "step: 73600, loss: 0.000004, accuracy: 1.000000\n",
      "step: 73700, loss: 0.000003, accuracy: 1.000000\n",
      "step: 73800, loss: 0.000004, accuracy: 1.000000\n",
      "step: 73900, loss: 0.000010, accuracy: 1.000000\n",
      "step: 74000, loss: 0.000003, accuracy: 1.000000\n",
      "step: 74100, loss: 0.000007, accuracy: 1.000000\n",
      "step: 74200, loss: 0.000004, accuracy: 1.000000\n",
      "step: 74300, loss: 0.000001, accuracy: 1.000000\n",
      "step: 74400, loss: 0.000008, accuracy: 1.000000\n",
      "step: 74500, loss: 0.000003, accuracy: 1.000000\n",
      "step: 74600, loss: 0.000004, accuracy: 1.000000\n",
      "step: 74700, loss: 0.000004, accuracy: 1.000000\n",
      "step: 74800, loss: 0.000003, accuracy: 1.000000\n",
      "step: 74900, loss: 0.000003, accuracy: 1.000000\n",
      "step: 75000, loss: 0.000002, accuracy: 1.000000\n",
      "step: 75100, loss: 0.000005, accuracy: 1.000000\n",
      "step: 75200, loss: 0.000001, accuracy: 1.000000\n",
      "step: 75300, loss: 0.000005, accuracy: 1.000000\n",
      "step: 75400, loss: 0.000003, accuracy: 1.000000\n",
      "step: 75500, loss: 0.000004, accuracy: 1.000000\n",
      "step: 75600, loss: 0.000003, accuracy: 1.000000\n",
      "step: 75700, loss: 0.000004, accuracy: 1.000000\n",
      "step: 75800, loss: 0.000002, accuracy: 1.000000\n",
      "step: 75900, loss: 0.000006, accuracy: 1.000000\n",
      "step: 76000, loss: 0.000002, accuracy: 1.000000\n",
      "step: 76100, loss: 0.000003, accuracy: 1.000000\n",
      "step: 76200, loss: 0.000004, accuracy: 1.000000\n",
      "step: 76300, loss: 0.000005, accuracy: 1.000000\n",
      "step: 76400, loss: 0.000002, accuracy: 1.000000\n",
      "step: 76500, loss: 0.000002, accuracy: 1.000000\n",
      "step: 76600, loss: 0.000003, accuracy: 1.000000\n",
      "step: 76700, loss: 0.000002, accuracy: 1.000000\n",
      "step: 76800, loss: 0.000001, accuracy: 1.000000\n",
      "step: 76900, loss: 0.000002, accuracy: 1.000000\n",
      "step: 77000, loss: 0.000002, accuracy: 1.000000\n",
      "step: 77100, loss: 0.000002, accuracy: 1.000000\n",
      "step: 77200, loss: 0.000003, accuracy: 1.000000\n",
      "step: 77300, loss: 0.000002, accuracy: 1.000000\n",
      "step: 77400, loss: 0.000002, accuracy: 1.000000\n",
      "step: 77500, loss: 0.000002, accuracy: 1.000000\n",
      "step: 77600, loss: 0.000002, accuracy: 1.000000\n",
      "step: 77700, loss: 0.000001, accuracy: 1.000000\n",
      "step: 77800, loss: 0.000002, accuracy: 1.000000\n",
      "step: 77900, loss: 0.000002, accuracy: 1.000000\n",
      "step: 78000, loss: 0.000001, accuracy: 1.000000\n",
      "step: 78100, loss: 0.000003, accuracy: 1.000000\n",
      "step: 78200, loss: 0.000002, accuracy: 1.000000\n",
      "step: 78300, loss: 0.000002, accuracy: 1.000000\n",
      "step: 78400, loss: 0.000002, accuracy: 1.000000\n",
      "step: 78500, loss: 0.000001, accuracy: 1.000000\n",
      "step: 78600, loss: 0.000001, accuracy: 1.000000\n",
      "step: 78700, loss: 0.000003, accuracy: 1.000000\n",
      "step: 78800, loss: 0.000002, accuracy: 1.000000\n",
      "step: 78900, loss: 0.000001, accuracy: 1.000000\n",
      "step: 79000, loss: 0.000001, accuracy: 1.000000\n",
      "step: 79100, loss: 0.000001, accuracy: 1.000000\n",
      "step: 79200, loss: 0.000001, accuracy: 1.000000\n",
      "step: 79300, loss: 0.000001, accuracy: 1.000000\n",
      "step: 79400, loss: 0.000000, accuracy: 1.000000\n",
      "step: 79500, loss: 0.000001, accuracy: 1.000000\n",
      "step: 79600, loss: 0.000002, accuracy: 1.000000\n",
      "step: 79700, loss: 0.000001, accuracy: 1.000000\n",
      "step: 79800, loss: 0.000001, accuracy: 1.000000\n",
      "step: 79900, loss: 0.000001, accuracy: 1.000000\n",
      "step: 80000, loss: 0.000001, accuracy: 1.000000\n",
      "step: 80100, loss: 0.000002, accuracy: 1.000000\n",
      "step: 80200, loss: 0.000001, accuracy: 1.000000\n",
      "step: 80300, loss: 0.000001, accuracy: 1.000000\n",
      "step: 80400, loss: 0.000001, accuracy: 1.000000\n",
      "step: 80500, loss: 0.000001, accuracy: 1.000000\n",
      "step: 80600, loss: 0.000000, accuracy: 1.000000\n",
      "step: 80700, loss: 0.000000, accuracy: 1.000000\n",
      "step: 80800, loss: 0.000001, accuracy: 1.000000\n",
      "step: 80900, loss: 0.000000, accuracy: 1.000000\n",
      "step: 81000, loss: 0.000001, accuracy: 1.000000\n",
      "step: 81100, loss: 0.000001, accuracy: 1.000000\n",
      "step: 81200, loss: 0.000000, accuracy: 1.000000\n",
      "step: 81300, loss: 0.000001, accuracy: 1.000000\n",
      "step: 81400, loss: 0.000000, accuracy: 1.000000\n",
      "step: 81500, loss: 0.000002, accuracy: 1.000000\n",
      "step: 81600, loss: 0.000001, accuracy: 1.000000\n",
      "step: 81700, loss: 0.000001, accuracy: 1.000000\n",
      "step: 81800, loss: 0.000001, accuracy: 1.000000\n",
      "step: 81900, loss: 0.000001, accuracy: 1.000000\n",
      "step: 82000, loss: 0.000000, accuracy: 1.000000\n",
      "step: 82100, loss: 0.000001, accuracy: 1.000000\n",
      "step: 82200, loss: 0.000001, accuracy: 1.000000\n",
      "step: 82300, loss: 0.000001, accuracy: 1.000000\n",
      "step: 82400, loss: 0.000001, accuracy: 1.000000\n",
      "step: 82500, loss: 0.000001, accuracy: 1.000000\n",
      "step: 82600, loss: 0.000001, accuracy: 1.000000\n",
      "step: 82700, loss: 0.000001, accuracy: 1.000000\n",
      "step: 82800, loss: 0.000001, accuracy: 1.000000\n",
      "step: 82900, loss: 0.000000, accuracy: 1.000000\n",
      "step: 83000, loss: 0.000001, accuracy: 1.000000\n",
      "step: 83100, loss: 0.000001, accuracy: 1.000000\n",
      "step: 83200, loss: 0.000001, accuracy: 1.000000\n",
      "step: 83300, loss: 0.000000, accuracy: 1.000000\n",
      "step: 83400, loss: 0.000000, accuracy: 1.000000\n",
      "step: 83500, loss: 0.000000, accuracy: 1.000000\n",
      "step: 83600, loss: 0.464027, accuracy: 1.000000\n",
      "step: 83700, loss: 0.465366, accuracy: 1.000000\n",
      "step: 83800, loss: 0.011593, accuracy: 1.000000\n",
      "step: 83900, loss: 0.017777, accuracy: 1.000000\n",
      "step: 84000, loss: 0.013203, accuracy: 1.000000\n",
      "step: 84100, loss: 0.007576, accuracy: 1.000000\n",
      "step: 84200, loss: 0.005714, accuracy: 1.000000\n",
      "step: 84300, loss: 0.000236, accuracy: 1.000000\n",
      "step: 84400, loss: 0.002299, accuracy: 1.000000\n",
      "step: 84500, loss: 0.002144, accuracy: 1.000000\n",
      "step: 84600, loss: 0.001640, accuracy: 1.000000\n",
      "step: 84700, loss: 0.000249, accuracy: 1.000000\n",
      "step: 84800, loss: 0.000304, accuracy: 1.000000\n",
      "step: 84900, loss: 0.000034, accuracy: 1.000000\n",
      "step: 85000, loss: 0.000039, accuracy: 1.000000\n",
      "step: 85100, loss: 0.002549, accuracy: 1.000000\n",
      "step: 85200, loss: 0.000294, accuracy: 1.000000\n",
      "step: 85300, loss: 0.001033, accuracy: 1.000000\n",
      "step: 85400, loss: 0.000033, accuracy: 1.000000\n",
      "step: 85500, loss: 0.000088, accuracy: 1.000000\n",
      "step: 85600, loss: 0.001665, accuracy: 1.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 85700, loss: 0.001184, accuracy: 1.000000\n",
      "step: 85800, loss: 0.000192, accuracy: 1.000000\n",
      "step: 85900, loss: 0.000369, accuracy: 1.000000\n",
      "step: 86000, loss: 0.001361, accuracy: 1.000000\n",
      "step: 86100, loss: 0.000644, accuracy: 1.000000\n",
      "step: 86200, loss: 0.000481, accuracy: 1.000000\n",
      "step: 86300, loss: 0.000351, accuracy: 1.000000\n",
      "step: 86400, loss: 0.000391, accuracy: 1.000000\n",
      "step: 86500, loss: 0.000241, accuracy: 1.000000\n",
      "step: 86600, loss: 0.000129, accuracy: 1.000000\n",
      "step: 86700, loss: 0.000207, accuracy: 1.000000\n",
      "step: 86800, loss: 0.000302, accuracy: 1.000000\n",
      "step: 86900, loss: 0.001050, accuracy: 1.000000\n",
      "step: 87000, loss: 0.000310, accuracy: 1.000000\n",
      "step: 87100, loss: 0.001295, accuracy: 1.000000\n",
      "step: 87200, loss: 0.000859, accuracy: 1.000000\n",
      "step: 87300, loss: 0.000499, accuracy: 1.000000\n",
      "step: 87400, loss: 0.000713, accuracy: 1.000000\n",
      "step: 87500, loss: 0.000578, accuracy: 1.000000\n",
      "step: 87600, loss: 0.000724, accuracy: 1.000000\n",
      "step: 87700, loss: 0.000282, accuracy: 1.000000\n",
      "step: 87800, loss: 0.000743, accuracy: 1.000000\n",
      "step: 87900, loss: 0.000026, accuracy: 1.000000\n",
      "step: 88000, loss: 0.000206, accuracy: 1.000000\n",
      "step: 88100, loss: 0.000445, accuracy: 1.000000\n",
      "step: 88200, loss: 0.000083, accuracy: 1.000000\n",
      "step: 88300, loss: 0.000385, accuracy: 1.000000\n",
      "step: 88400, loss: 0.000589, accuracy: 1.000000\n",
      "step: 88500, loss: 0.000228, accuracy: 1.000000\n",
      "step: 88600, loss: 0.000286, accuracy: 1.000000\n",
      "step: 88700, loss: 0.000224, accuracy: 1.000000\n",
      "step: 88800, loss: 0.000371, accuracy: 1.000000\n",
      "step: 88900, loss: 0.000581, accuracy: 1.000000\n",
      "step: 89000, loss: 0.000146, accuracy: 1.000000\n",
      "step: 89100, loss: 0.000051, accuracy: 1.000000\n",
      "step: 89200, loss: 0.000119, accuracy: 1.000000\n",
      "step: 89300, loss: 0.000112, accuracy: 1.000000\n",
      "step: 89400, loss: 0.000533, accuracy: 1.000000\n",
      "step: 89500, loss: 0.000110, accuracy: 1.000000\n",
      "step: 89600, loss: 0.000444, accuracy: 1.000000\n",
      "step: 89700, loss: 0.000052, accuracy: 1.000000\n",
      "step: 89800, loss: 0.000107, accuracy: 1.000000\n",
      "step: 89900, loss: 0.000288, accuracy: 1.000000\n",
      "step: 90000, loss: 0.000148, accuracy: 1.000000\n",
      "step: 90100, loss: 0.000312, accuracy: 1.000000\n",
      "step: 90200, loss: 0.000163, accuracy: 1.000000\n",
      "step: 90300, loss: 0.000191, accuracy: 1.000000\n",
      "step: 90400, loss: 0.000102, accuracy: 1.000000\n",
      "step: 90500, loss: 0.000056, accuracy: 1.000000\n",
      "step: 90600, loss: 0.000208, accuracy: 1.000000\n",
      "step: 90700, loss: 0.000091, accuracy: 1.000000\n",
      "step: 90800, loss: 0.000022, accuracy: 1.000000\n",
      "step: 90900, loss: 0.000080, accuracy: 1.000000\n",
      "step: 91000, loss: 0.000272, accuracy: 1.000000\n",
      "step: 91100, loss: 0.000147, accuracy: 1.000000\n",
      "step: 91200, loss: 0.000057, accuracy: 1.000000\n",
      "step: 91300, loss: 0.000065, accuracy: 1.000000\n",
      "step: 91400, loss: 0.000056, accuracy: 1.000000\n",
      "step: 91500, loss: 0.000073, accuracy: 1.000000\n",
      "step: 91600, loss: 0.000068, accuracy: 1.000000\n",
      "step: 91700, loss: 0.000113, accuracy: 1.000000\n",
      "step: 91800, loss: 0.000145, accuracy: 1.000000\n",
      "step: 91900, loss: 0.000055, accuracy: 1.000000\n",
      "step: 92000, loss: 0.000097, accuracy: 1.000000\n",
      "step: 92100, loss: 0.000020, accuracy: 1.000000\n",
      "step: 92200, loss: 0.000082, accuracy: 1.000000\n",
      "step: 92300, loss: 0.000061, accuracy: 1.000000\n",
      "step: 92400, loss: 0.000055, accuracy: 1.000000\n",
      "step: 92500, loss: 0.000185, accuracy: 1.000000\n",
      "step: 92600, loss: 0.000045, accuracy: 1.000000\n",
      "step: 92700, loss: 0.000037, accuracy: 1.000000\n",
      "step: 92800, loss: 0.000064, accuracy: 1.000000\n",
      "step: 92900, loss: 0.000015, accuracy: 1.000000\n",
      "step: 93000, loss: 0.000029, accuracy: 1.000000\n",
      "step: 93100, loss: 0.000061, accuracy: 1.000000\n",
      "step: 93200, loss: 0.000107, accuracy: 1.000000\n",
      "step: 93300, loss: 0.000045, accuracy: 1.000000\n",
      "step: 93400, loss: 0.000068, accuracy: 1.000000\n",
      "step: 93500, loss: 0.000050, accuracy: 1.000000\n",
      "step: 93600, loss: 0.000050, accuracy: 1.000000\n",
      "step: 93700, loss: 0.000018, accuracy: 1.000000\n",
      "step: 93800, loss: 0.000035, accuracy: 1.000000\n",
      "step: 93900, loss: 0.000046, accuracy: 1.000000\n",
      "step: 94000, loss: 0.000083, accuracy: 1.000000\n",
      "step: 94100, loss: 0.000036, accuracy: 1.000000\n",
      "step: 94200, loss: 0.000026, accuracy: 1.000000\n",
      "step: 94300, loss: 0.000013, accuracy: 1.000000\n",
      "step: 94400, loss: 0.000030, accuracy: 1.000000\n",
      "step: 94500, loss: 0.000009, accuracy: 1.000000\n",
      "step: 94600, loss: 0.000039, accuracy: 1.000000\n",
      "step: 94700, loss: 0.000021, accuracy: 1.000000\n",
      "step: 94800, loss: 0.000016, accuracy: 1.000000\n",
      "step: 94900, loss: 0.000043, accuracy: 1.000000\n",
      "step: 95000, loss: 0.000014, accuracy: 1.000000\n",
      "step: 95100, loss: 0.000019, accuracy: 1.000000\n",
      "step: 95200, loss: 0.000021, accuracy: 1.000000\n",
      "step: 95300, loss: 0.000017, accuracy: 1.000000\n",
      "step: 95400, loss: 0.000036, accuracy: 1.000000\n",
      "step: 95500, loss: 0.000014, accuracy: 1.000000\n",
      "step: 95600, loss: 0.000052, accuracy: 1.000000\n",
      "step: 95700, loss: 0.000033, accuracy: 1.000000\n",
      "step: 95800, loss: 0.000017, accuracy: 1.000000\n",
      "step: 95900, loss: 0.000039, accuracy: 1.000000\n",
      "step: 96000, loss: 0.000031, accuracy: 1.000000\n",
      "step: 96100, loss: 0.000015, accuracy: 1.000000\n",
      "step: 96200, loss: 0.000010, accuracy: 1.000000\n",
      "step: 96300, loss: 0.000010, accuracy: 1.000000\n",
      "step: 96400, loss: 0.000006, accuracy: 1.000000\n",
      "step: 96500, loss: 0.000009, accuracy: 1.000000\n",
      "step: 96600, loss: 0.000011, accuracy: 1.000000\n",
      "step: 96700, loss: 0.000012, accuracy: 1.000000\n",
      "step: 96800, loss: 0.000012, accuracy: 1.000000\n",
      "step: 96900, loss: 0.000015, accuracy: 1.000000\n",
      "step: 97000, loss: 0.000028, accuracy: 1.000000\n",
      "step: 97100, loss: 0.000011, accuracy: 1.000000\n",
      "step: 97200, loss: 0.000020, accuracy: 1.000000\n",
      "step: 97300, loss: 0.000025, accuracy: 1.000000\n",
      "step: 97400, loss: 0.000012, accuracy: 1.000000\n",
      "step: 97500, loss: 0.000009, accuracy: 1.000000\n",
      "step: 97600, loss: 0.000008, accuracy: 1.000000\n",
      "step: 97700, loss: 0.000010, accuracy: 1.000000\n",
      "step: 97800, loss: 0.000005, accuracy: 1.000000\n",
      "step: 97900, loss: 0.000007, accuracy: 1.000000\n",
      "step: 98000, loss: 0.000008, accuracy: 1.000000\n",
      "step: 98100, loss: 0.000005, accuracy: 1.000000\n",
      "step: 98200, loss: 0.000012, accuracy: 1.000000\n",
      "step: 98300, loss: 0.000005, accuracy: 1.000000\n",
      "step: 98400, loss: 0.000006, accuracy: 1.000000\n",
      "step: 98500, loss: 0.000005, accuracy: 1.000000\n",
      "step: 98600, loss: 0.000011, accuracy: 1.000000\n",
      "step: 98700, loss: 0.000005, accuracy: 1.000000\n",
      "step: 98800, loss: 0.000008, accuracy: 1.000000\n",
      "step: 98900, loss: 0.000008, accuracy: 1.000000\n",
      "step: 99000, loss: 0.000009, accuracy: 1.000000\n",
      "step: 99100, loss: 0.000010, accuracy: 1.000000\n",
      "step: 99200, loss: 0.000008, accuracy: 1.000000\n",
      "step: 99300, loss: 0.000006, accuracy: 1.000000\n",
      "step: 99400, loss: 0.000003, accuracy: 1.000000\n",
      "step: 99500, loss: 0.000002, accuracy: 1.000000\n",
      "step: 99600, loss: 0.000006, accuracy: 1.000000\n",
      "step: 99700, loss: 0.000003, accuracy: 1.000000\n",
      "step: 99800, loss: 0.000006, accuracy: 1.000000\n",
      "step: 99900, loss: 0.000004, accuracy: 1.000000\n",
      "step: 100000, loss: 0.000004, accuracy: 1.000000\n",
      "step: 100100, loss: 0.000002, accuracy: 1.000000\n",
      "step: 100200, loss: 0.000003, accuracy: 1.000000\n",
      "step: 100300, loss: 0.000003, accuracy: 1.000000\n",
      "step: 100400, loss: 0.000007, accuracy: 1.000000\n",
      "step: 100500, loss: 0.000003, accuracy: 1.000000\n",
      "step: 100600, loss: 0.000002, accuracy: 1.000000\n",
      "step: 100700, loss: 0.000004, accuracy: 1.000000\n",
      "step: 100800, loss: 0.000003, accuracy: 1.000000\n",
      "step: 100900, loss: 0.000005, accuracy: 1.000000\n",
      "step: 101000, loss: 0.000002, accuracy: 1.000000\n",
      "step: 101100, loss: 0.000003, accuracy: 1.000000\n",
      "step: 101200, loss: 0.000005, accuracy: 1.000000\n",
      "step: 101300, loss: 0.000004, accuracy: 1.000000\n",
      "step: 101400, loss: 0.000004, accuracy: 1.000000\n",
      "step: 101500, loss: 0.000001, accuracy: 1.000000\n",
      "step: 101600, loss: 0.000001, accuracy: 1.000000\n",
      "step: 101700, loss: 0.000000, accuracy: 1.000000\n",
      "step: 101800, loss: 0.000002, accuracy: 1.000000\n",
      "step: 101900, loss: 0.000003, accuracy: 1.000000\n",
      "step: 102000, loss: 0.000002, accuracy: 1.000000\n",
      "step: 102100, loss: 0.000002, accuracy: 1.000000\n",
      "step: 102200, loss: 0.000002, accuracy: 1.000000\n",
      "step: 102300, loss: 0.000001, accuracy: 1.000000\n",
      "step: 102400, loss: 0.000002, accuracy: 1.000000\n",
      "step: 102500, loss: 0.000003, accuracy: 1.000000\n",
      "step: 102600, loss: 0.000001, accuracy: 1.000000\n",
      "step: 102700, loss: 0.000002, accuracy: 1.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 102800, loss: 0.000002, accuracy: 1.000000\n",
      "step: 102900, loss: 0.000002, accuracy: 1.000000\n",
      "step: 103000, loss: 0.000002, accuracy: 1.000000\n",
      "step: 103100, loss: 0.000001, accuracy: 1.000000\n",
      "step: 103200, loss: 0.000001, accuracy: 1.000000\n",
      "step: 103300, loss: 0.000001, accuracy: 1.000000\n",
      "step: 103400, loss: 0.000001, accuracy: 1.000000\n",
      "step: 103500, loss: 0.000001, accuracy: 1.000000\n",
      "step: 103600, loss: 0.000003, accuracy: 1.000000\n",
      "step: 103700, loss: 0.000001, accuracy: 1.000000\n",
      "step: 103800, loss: 0.000001, accuracy: 1.000000\n",
      "step: 103900, loss: 0.000001, accuracy: 1.000000\n",
      "step: 104000, loss: 0.000001, accuracy: 1.000000\n",
      "step: 104100, loss: 0.000001, accuracy: 1.000000\n",
      "step: 104200, loss: 0.000003, accuracy: 1.000000\n",
      "step: 104300, loss: 0.000001, accuracy: 1.000000\n",
      "step: 104400, loss: 0.000002, accuracy: 1.000000\n",
      "step: 104500, loss: 0.000002, accuracy: 1.000000\n",
      "step: 104600, loss: 0.000002, accuracy: 1.000000\n",
      "step: 104700, loss: 0.000001, accuracy: 1.000000\n",
      "step: 104800, loss: 0.000002, accuracy: 1.000000\n",
      "step: 104900, loss: 0.000002, accuracy: 1.000000\n",
      "step: 105000, loss: 0.000002, accuracy: 1.000000\n",
      "step: 105100, loss: 0.000001, accuracy: 1.000000\n",
      "step: 105200, loss: 0.000002, accuracy: 1.000000\n",
      "step: 105300, loss: 0.000001, accuracy: 1.000000\n",
      "step: 105400, loss: 0.000001, accuracy: 1.000000\n",
      "step: 105500, loss: 0.000002, accuracy: 1.000000\n",
      "step: 105600, loss: 0.000001, accuracy: 1.000000\n",
      "step: 105700, loss: 0.000001, accuracy: 1.000000\n",
      "step: 105800, loss: 0.000000, accuracy: 1.000000\n",
      "step: 105900, loss: 0.000001, accuracy: 1.000000\n",
      "step: 106000, loss: 0.000001, accuracy: 1.000000\n",
      "step: 106100, loss: 0.000000, accuracy: 1.000000\n",
      "step: 106200, loss: 0.000001, accuracy: 1.000000\n",
      "step: 106300, loss: 0.000001, accuracy: 1.000000\n",
      "step: 106400, loss: 0.000001, accuracy: 1.000000\n",
      "step: 106500, loss: 0.000001, accuracy: 1.000000\n",
      "step: 106600, loss: 0.000001, accuracy: 1.000000\n",
      "step: 106700, loss: 0.000001, accuracy: 1.000000\n",
      "step: 106800, loss: 0.000003, accuracy: 1.000000\n",
      "step: 106900, loss: 0.000001, accuracy: 1.000000\n",
      "step: 107000, loss: 0.000001, accuracy: 1.000000\n",
      "step: 107100, loss: 0.000001, accuracy: 1.000000\n",
      "step: 107200, loss: 0.000001, accuracy: 1.000000\n",
      "step: 107300, loss: 0.000000, accuracy: 1.000000\n",
      "step: 107400, loss: 0.000001, accuracy: 1.000000\n",
      "step: 107500, loss: 0.000001, accuracy: 1.000000\n",
      "step: 107600, loss: 0.000001, accuracy: 1.000000\n",
      "step: 107700, loss: 0.000000, accuracy: 1.000000\n",
      "step: 107800, loss: 0.000001, accuracy: 1.000000\n",
      "step: 107900, loss: 0.000001, accuracy: 1.000000\n",
      "step: 108000, loss: 0.000000, accuracy: 1.000000\n",
      "step: 108100, loss: 0.000001, accuracy: 1.000000\n",
      "step: 108200, loss: 0.000001, accuracy: 1.000000\n",
      "step: 108300, loss: 0.000001, accuracy: 1.000000\n",
      "step: 108400, loss: 0.000001, accuracy: 1.000000\n",
      "step: 108500, loss: 0.000001, accuracy: 1.000000\n",
      "step: 108600, loss: 0.000001, accuracy: 1.000000\n",
      "step: 108700, loss: 0.000001, accuracy: 1.000000\n",
      "step: 108800, loss: 0.000000, accuracy: 1.000000\n",
      "step: 108900, loss: 0.000000, accuracy: 1.000000\n",
      "step: 109000, loss: 0.000001, accuracy: 1.000000\n",
      "step: 109100, loss: 0.000001, accuracy: 1.000000\n",
      "step: 109200, loss: 0.000001, accuracy: 1.000000\n",
      "step: 109300, loss: 0.000001, accuracy: 1.000000\n",
      "step: 109400, loss: 0.000002, accuracy: 1.000000\n",
      "step: 109500, loss: 1.303098, accuracy: 1.000000\n",
      "step: 109600, loss: 0.204071, accuracy: 1.000000\n",
      "step: 109700, loss: 0.003431, accuracy: 1.000000\n",
      "step: 109800, loss: 0.001142, accuracy: 1.000000\n",
      "step: 109900, loss: 0.000711, accuracy: 1.000000\n",
      "step: 110000, loss: 0.000926, accuracy: 1.000000\n",
      "step: 110100, loss: 0.001864, accuracy: 1.000000\n",
      "step: 110200, loss: 0.001398, accuracy: 1.000000\n",
      "step: 110300, loss: 0.000982, accuracy: 1.000000\n",
      "step: 110400, loss: 0.000697, accuracy: 1.000000\n",
      "step: 110500, loss: 0.005641, accuracy: 1.000000\n",
      "step: 110600, loss: 0.000694, accuracy: 1.000000\n",
      "step: 110700, loss: 0.001501, accuracy: 1.000000\n",
      "step: 110800, loss: 0.001317, accuracy: 1.000000\n",
      "step: 110900, loss: 0.000083, accuracy: 1.000000\n",
      "step: 111000, loss: 0.001566, accuracy: 1.000000\n",
      "step: 111100, loss: 0.002477, accuracy: 1.000000\n",
      "step: 111200, loss: 0.000029, accuracy: 1.000000\n",
      "step: 111300, loss: 0.002904, accuracy: 1.000000\n",
      "step: 111400, loss: 0.002857, accuracy: 1.000000\n",
      "step: 111500, loss: 0.000180, accuracy: 1.000000\n",
      "step: 111600, loss: 0.000687, accuracy: 1.000000\n",
      "step: 111700, loss: 0.000104, accuracy: 1.000000\n",
      "step: 111800, loss: 0.000057, accuracy: 1.000000\n",
      "step: 111900, loss: 0.001525, accuracy: 1.000000\n",
      "step: 112000, loss: 0.000931, accuracy: 1.000000\n",
      "step: 112100, loss: 0.001194, accuracy: 1.000000\n",
      "step: 112200, loss: 0.000925, accuracy: 1.000000\n",
      "step: 112300, loss: 0.000816, accuracy: 1.000000\n",
      "step: 112400, loss: 0.000054, accuracy: 1.000000\n",
      "step: 112500, loss: 0.000249, accuracy: 1.000000\n",
      "step: 112600, loss: 0.000481, accuracy: 1.000000\n",
      "step: 112700, loss: 0.000141, accuracy: 1.000000\n",
      "step: 112800, loss: 0.000645, accuracy: 1.000000\n",
      "step: 112900, loss: 0.000017, accuracy: 1.000000\n",
      "step: 113000, loss: 0.001134, accuracy: 1.000000\n",
      "step: 113100, loss: 0.000856, accuracy: 1.000000\n",
      "step: 113200, loss: 0.001955, accuracy: 1.000000\n",
      "step: 113300, loss: 0.000077, accuracy: 1.000000\n",
      "step: 113400, loss: 0.000151, accuracy: 1.000000\n",
      "step: 113500, loss: 0.000055, accuracy: 1.000000\n",
      "step: 113600, loss: 0.000255, accuracy: 1.000000\n",
      "step: 113700, loss: 0.000013, accuracy: 1.000000\n",
      "step: 113800, loss: 0.000207, accuracy: 1.000000\n",
      "step: 113900, loss: 0.000274, accuracy: 1.000000\n",
      "step: 114000, loss: 0.000177, accuracy: 1.000000\n",
      "step: 114100, loss: 0.000195, accuracy: 1.000000\n",
      "step: 114200, loss: 0.000026, accuracy: 1.000000\n",
      "step: 114300, loss: 0.000567, accuracy: 1.000000\n",
      "step: 114400, loss: 0.000591, accuracy: 1.000000\n",
      "step: 114500, loss: 0.000322, accuracy: 1.000000\n",
      "step: 114600, loss: 0.000434, accuracy: 1.000000\n",
      "step: 114700, loss: 0.000471, accuracy: 1.000000\n",
      "step: 114800, loss: 0.000278, accuracy: 1.000000\n",
      "step: 114900, loss: 0.000325, accuracy: 1.000000\n",
      "step: 115000, loss: 0.000125, accuracy: 1.000000\n",
      "step: 115100, loss: 0.000265, accuracy: 1.000000\n",
      "step: 115200, loss: 0.000143, accuracy: 1.000000\n",
      "step: 115300, loss: 0.000380, accuracy: 1.000000\n",
      "step: 115400, loss: 0.000075, accuracy: 1.000000\n",
      "step: 115500, loss: 0.000118, accuracy: 1.000000\n",
      "step: 115600, loss: 0.000148, accuracy: 1.000000\n",
      "step: 115700, loss: 0.000292, accuracy: 1.000000\n",
      "step: 115800, loss: 0.000111, accuracy: 1.000000\n",
      "step: 115900, loss: 0.000044, accuracy: 1.000000\n",
      "step: 116000, loss: 0.000084, accuracy: 1.000000\n",
      "step: 116100, loss: 0.000119, accuracy: 1.000000\n",
      "step: 116200, loss: 0.000243, accuracy: 1.000000\n",
      "step: 116300, loss: 0.000048, accuracy: 1.000000\n",
      "step: 116400, loss: 0.000113, accuracy: 1.000000\n",
      "step: 116500, loss: 0.000014, accuracy: 1.000000\n",
      "step: 116600, loss: 0.000099, accuracy: 1.000000\n",
      "step: 116700, loss: 0.000121, accuracy: 1.000000\n",
      "step: 116800, loss: 0.000087, accuracy: 1.000000\n",
      "step: 116900, loss: 0.000016, accuracy: 1.000000\n",
      "step: 117000, loss: 0.000218, accuracy: 1.000000\n",
      "step: 117100, loss: 0.000214, accuracy: 1.000000\n",
      "step: 117200, loss: 0.000316, accuracy: 1.000000\n",
      "step: 117300, loss: 0.000060, accuracy: 1.000000\n",
      "step: 117400, loss: 0.000090, accuracy: 1.000000\n",
      "step: 117500, loss: 0.000046, accuracy: 1.000000\n",
      "step: 117600, loss: 0.000033, accuracy: 1.000000\n",
      "step: 117700, loss: 0.000078, accuracy: 1.000000\n",
      "step: 117800, loss: 0.000167, accuracy: 1.000000\n",
      "step: 117900, loss: 0.000031, accuracy: 1.000000\n",
      "step: 118000, loss: 0.000032, accuracy: 1.000000\n",
      "step: 118100, loss: 0.000129, accuracy: 1.000000\n",
      "step: 118200, loss: 0.000078, accuracy: 1.000000\n",
      "step: 118300, loss: 0.000093, accuracy: 1.000000\n",
      "step: 118400, loss: 0.000042, accuracy: 1.000000\n",
      "step: 118500, loss: 0.000053, accuracy: 1.000000\n",
      "step: 118600, loss: 0.000094, accuracy: 1.000000\n",
      "step: 118700, loss: 0.000117, accuracy: 1.000000\n",
      "step: 118800, loss: 0.000010, accuracy: 1.000000\n",
      "step: 118900, loss: 0.000090, accuracy: 1.000000\n",
      "step: 119000, loss: 0.000066, accuracy: 1.000000\n",
      "step: 119100, loss: 0.000047, accuracy: 1.000000\n",
      "step: 119200, loss: 0.000073, accuracy: 1.000000\n",
      "step: 119300, loss: 0.000012, accuracy: 1.000000\n",
      "step: 119400, loss: 0.000041, accuracy: 1.000000\n",
      "step: 119500, loss: 0.000014, accuracy: 1.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 119600, loss: 0.000029, accuracy: 1.000000\n",
      "step: 119700, loss: 0.000069, accuracy: 1.000000\n",
      "step: 119800, loss: 0.000063, accuracy: 1.000000\n",
      "step: 119900, loss: 0.000019, accuracy: 1.000000\n",
      "step: 120000, loss: 0.000004, accuracy: 1.000000\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of DNN-softmax on the testset is 0.9789000153541565\n"
     ]
    }
   ],
   "source": [
    "model = DNN_softmax_and_SVM.DNN_softmax(num_classes = 10,  \n",
    "                    num_input = 70,    \n",
    "                    n_hidden_1 = 512,  \n",
    "                    n_hidden_2 = 512,\n",
    "                   )\n",
    "\n",
    "step_list, loss_list, acc_list = model.fit( x_train, y_train, \n",
    "                                            optimizer = tf.optimizers.Adam(learning_rate = 0.001),\n",
    "                                            training_steps = 400*300, \n",
    "                                            display_step = 100 , \n",
    "                                            batch_size = 200, \n",
    "                                          )\n",
    "\n",
    "plt.plot(step_list, acc_list )\n",
    "plt.title(f'model {model.name} on MNIST')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(step_list, loss_list )\n",
    "plt.title(f'model {model.name} on MNIST')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "model.evaluate(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 100, loss: 4125.399414, accuracy: 0.755000\n",
      "step: 200, loss: 3840.236084, accuracy: 0.905000\n",
      "step: 300, loss: 3901.189941, accuracy: 0.840000\n",
      "step: 400, loss: 3788.894043, accuracy: 0.925000\n",
      "step: 500, loss: 3875.228271, accuracy: 0.860000\n",
      "step: 600, loss: 3777.828857, accuracy: 0.920000\n",
      "step: 700, loss: 3837.136475, accuracy: 0.865000\n",
      "step: 800, loss: 3821.708252, accuracy: 0.880000\n",
      "step: 900, loss: 3803.525146, accuracy: 0.890000\n",
      "step: 1000, loss: 3809.780518, accuracy: 0.880000\n",
      "step: 1100, loss: 3846.418945, accuracy: 0.850000\n",
      "step: 1200, loss: 3785.922363, accuracy: 0.905000\n",
      "step: 1300, loss: 3831.094238, accuracy: 0.875000\n",
      "step: 1400, loss: 3819.683594, accuracy: 0.875000\n",
      "step: 1500, loss: 3782.903076, accuracy: 0.885000\n",
      "step: 1600, loss: 3814.693115, accuracy: 0.890000\n",
      "step: 1700, loss: 3776.013428, accuracy: 0.900000\n",
      "step: 1800, loss: 3796.181396, accuracy: 0.895000\n",
      "step: 1900, loss: 3787.035156, accuracy: 0.905000\n",
      "step: 2000, loss: 3768.015625, accuracy: 0.920000\n",
      "step: 2100, loss: 3719.271484, accuracy: 0.950000\n",
      "step: 2200, loss: 3797.185303, accuracy: 0.885000\n",
      "step: 2300, loss: 3764.252930, accuracy: 0.915000\n",
      "step: 2400, loss: 3743.657715, accuracy: 0.930000\n",
      "step: 2500, loss: 3778.770508, accuracy: 0.905000\n",
      "step: 2600, loss: 3701.304688, accuracy: 0.955000\n",
      "step: 2700, loss: 3762.966797, accuracy: 0.925000\n",
      "step: 2800, loss: 3766.748047, accuracy: 0.920000\n",
      "step: 2900, loss: 3808.552002, accuracy: 0.885000\n",
      "step: 3000, loss: 3733.007080, accuracy: 0.940000\n",
      "step: 3100, loss: 3762.368164, accuracy: 0.910000\n",
      "step: 3200, loss: 3739.631104, accuracy: 0.935000\n",
      "step: 3300, loss: 3763.556396, accuracy: 0.920000\n",
      "step: 3400, loss: 3809.599854, accuracy: 0.885000\n",
      "step: 3500, loss: 3784.643311, accuracy: 0.905000\n",
      "step: 3600, loss: 3754.981934, accuracy: 0.920000\n",
      "step: 3700, loss: 3699.941162, accuracy: 0.960000\n",
      "step: 3800, loss: 3747.440918, accuracy: 0.925000\n",
      "step: 3900, loss: 3771.251465, accuracy: 0.900000\n",
      "step: 4000, loss: 3752.123291, accuracy: 0.925000\n",
      "step: 4100, loss: 3773.157227, accuracy: 0.895000\n",
      "step: 4200, loss: 3740.543945, accuracy: 0.935000\n",
      "step: 4300, loss: 3726.389160, accuracy: 0.945000\n",
      "step: 4400, loss: 3778.803711, accuracy: 0.905000\n",
      "step: 4500, loss: 3684.411377, accuracy: 0.965000\n",
      "step: 4600, loss: 3744.650879, accuracy: 0.930000\n",
      "step: 4700, loss: 3727.469971, accuracy: 0.940000\n",
      "step: 4800, loss: 3689.695557, accuracy: 0.965000\n",
      "step: 4900, loss: 3741.879395, accuracy: 0.925000\n",
      "step: 5000, loss: 3721.906738, accuracy: 0.955000\n",
      "step: 5100, loss: 3708.447266, accuracy: 0.965000\n",
      "step: 5200, loss: 3700.197021, accuracy: 0.960000\n",
      "step: 5300, loss: 3743.359131, accuracy: 0.930000\n",
      "step: 5400, loss: 3710.829346, accuracy: 0.955000\n",
      "step: 5500, loss: 3728.140381, accuracy: 0.940000\n",
      "step: 5600, loss: 3757.998291, accuracy: 0.925000\n",
      "step: 5700, loss: 3664.792969, accuracy: 0.985000\n",
      "step: 5800, loss: 3696.693359, accuracy: 0.950000\n",
      "step: 5900, loss: 3734.290283, accuracy: 0.935000\n",
      "step: 6000, loss: 3708.210449, accuracy: 0.950000\n",
      "step: 6100, loss: 3691.423584, accuracy: 0.970000\n",
      "step: 6200, loss: 3720.622070, accuracy: 0.935000\n",
      "step: 6300, loss: 3710.107666, accuracy: 0.945000\n",
      "step: 6400, loss: 3712.487793, accuracy: 0.960000\n",
      "step: 6500, loss: 3712.296387, accuracy: 0.945000\n",
      "step: 6600, loss: 3701.685303, accuracy: 0.970000\n",
      "step: 6700, loss: 3675.485840, accuracy: 0.980000\n",
      "step: 6800, loss: 3688.351318, accuracy: 0.975000\n",
      "step: 6900, loss: 3677.924805, accuracy: 0.985000\n",
      "step: 7000, loss: 3720.624023, accuracy: 0.940000\n",
      "step: 7100, loss: 3677.446533, accuracy: 0.980000\n",
      "step: 7200, loss: 3691.876709, accuracy: 0.970000\n",
      "step: 7300, loss: 3708.329102, accuracy: 0.955000\n",
      "step: 7400, loss: 3702.968994, accuracy: 0.960000\n",
      "step: 7500, loss: 3689.787354, accuracy: 0.955000\n",
      "step: 7600, loss: 3708.191650, accuracy: 0.945000\n",
      "step: 7700, loss: 3734.960693, accuracy: 0.935000\n",
      "step: 7800, loss: 3681.838379, accuracy: 0.970000\n",
      "step: 7900, loss: 3702.145996, accuracy: 0.960000\n",
      "step: 8000, loss: 3713.932129, accuracy: 0.945000\n",
      "step: 8100, loss: 3680.272705, accuracy: 0.975000\n",
      "step: 8200, loss: 3694.458740, accuracy: 0.965000\n",
      "step: 8300, loss: 3686.443848, accuracy: 0.970000\n",
      "step: 8400, loss: 3683.891602, accuracy: 0.980000\n",
      "step: 8500, loss: 3724.189941, accuracy: 0.940000\n",
      "step: 8600, loss: 3723.957275, accuracy: 0.945000\n",
      "step: 8700, loss: 3686.481201, accuracy: 0.970000\n",
      "step: 8800, loss: 3682.370850, accuracy: 0.965000\n",
      "step: 8900, loss: 3678.663086, accuracy: 0.970000\n",
      "step: 9000, loss: 3672.442871, accuracy: 0.975000\n",
      "step: 9100, loss: 3726.012695, accuracy: 0.940000\n",
      "step: 9200, loss: 3691.230225, accuracy: 0.970000\n",
      "step: 9300, loss: 3673.344238, accuracy: 0.975000\n",
      "step: 9400, loss: 3661.697510, accuracy: 0.985000\n",
      "step: 9500, loss: 3680.079834, accuracy: 0.970000\n",
      "step: 9600, loss: 3683.887207, accuracy: 0.965000\n",
      "step: 9700, loss: 3680.787598, accuracy: 0.970000\n",
      "step: 9800, loss: 3707.827393, accuracy: 0.960000\n",
      "step: 9900, loss: 3657.994385, accuracy: 0.985000\n",
      "step: 10000, loss: 3699.583008, accuracy: 0.960000\n",
      "step: 10100, loss: 3687.879150, accuracy: 0.965000\n",
      "step: 10200, loss: 3668.898193, accuracy: 0.975000\n",
      "step: 10300, loss: 3688.849121, accuracy: 0.960000\n",
      "step: 10400, loss: 3686.911621, accuracy: 0.975000\n",
      "step: 10500, loss: 3689.070801, accuracy: 0.970000\n",
      "step: 10600, loss: 3656.761230, accuracy: 0.985000\n",
      "step: 10700, loss: 3676.427002, accuracy: 0.980000\n",
      "step: 10800, loss: 3649.987061, accuracy: 0.990000\n",
      "step: 10900, loss: 3686.598145, accuracy: 0.965000\n",
      "step: 11000, loss: 3693.563477, accuracy: 0.965000\n",
      "step: 11100, loss: 3674.286377, accuracy: 0.975000\n",
      "step: 11200, loss: 3690.349365, accuracy: 0.970000\n",
      "step: 11300, loss: 3704.766113, accuracy: 0.960000\n",
      "step: 11400, loss: 3653.270996, accuracy: 0.985000\n",
      "step: 11500, loss: 3685.521484, accuracy: 0.970000\n",
      "step: 11600, loss: 3662.575928, accuracy: 0.975000\n",
      "step: 11700, loss: 3668.629395, accuracy: 0.980000\n",
      "step: 11800, loss: 3662.574951, accuracy: 0.980000\n",
      "step: 11900, loss: 3655.589600, accuracy: 0.980000\n",
      "step: 12000, loss: 3658.856445, accuracy: 0.980000\n",
      "step: 12100, loss: 3662.860107, accuracy: 0.980000\n",
      "step: 12200, loss: 3663.468994, accuracy: 0.985000\n",
      "step: 12300, loss: 3657.606445, accuracy: 0.985000\n",
      "step: 12400, loss: 3689.800537, accuracy: 0.965000\n",
      "step: 12500, loss: 3652.744385, accuracy: 0.990000\n",
      "step: 12600, loss: 3664.492676, accuracy: 0.985000\n",
      "step: 12700, loss: 3673.212402, accuracy: 0.975000\n",
      "step: 12800, loss: 3655.007568, accuracy: 0.980000\n",
      "step: 12900, loss: 3645.102295, accuracy: 0.995000\n",
      "step: 13000, loss: 3644.241455, accuracy: 0.995000\n",
      "step: 13100, loss: 3666.446533, accuracy: 0.980000\n",
      "step: 13200, loss: 3655.766846, accuracy: 0.990000\n",
      "step: 13300, loss: 3669.352051, accuracy: 0.975000\n",
      "step: 13400, loss: 3636.436279, accuracy: 0.995000\n",
      "step: 13500, loss: 3666.617432, accuracy: 0.980000\n",
      "step: 13600, loss: 3660.188721, accuracy: 0.985000\n",
      "step: 13700, loss: 3645.473389, accuracy: 0.990000\n",
      "step: 13800, loss: 3664.461426, accuracy: 0.980000\n",
      "step: 13900, loss: 3657.678955, accuracy: 0.980000\n",
      "step: 14000, loss: 3649.106934, accuracy: 0.985000\n",
      "step: 14100, loss: 3644.699707, accuracy: 0.995000\n",
      "step: 14200, loss: 3666.308594, accuracy: 0.980000\n",
      "step: 14300, loss: 3668.457520, accuracy: 0.975000\n",
      "step: 14400, loss: 3659.237305, accuracy: 0.985000\n",
      "step: 14500, loss: 3643.972656, accuracy: 0.990000\n",
      "step: 14600, loss: 3657.093994, accuracy: 0.985000\n",
      "step: 14700, loss: 3642.321289, accuracy: 0.990000\n",
      "step: 14800, loss: 3662.887695, accuracy: 0.980000\n",
      "step: 14900, loss: 3645.008301, accuracy: 0.990000\n",
      "step: 15000, loss: 3641.551270, accuracy: 0.995000\n",
      "step: 15100, loss: 3647.296143, accuracy: 0.990000\n",
      "step: 15200, loss: 3647.423584, accuracy: 0.990000\n",
      "step: 15300, loss: 3629.877930, accuracy: 1.000000\n",
      "step: 15400, loss: 3640.075439, accuracy: 0.995000\n",
      "step: 15500, loss: 3648.652100, accuracy: 0.990000\n",
      "step: 15600, loss: 3640.772949, accuracy: 0.990000\n",
      "step: 15700, loss: 3655.837402, accuracy: 0.980000\n",
      "step: 15800, loss: 3629.741455, accuracy: 0.995000\n",
      "step: 15900, loss: 3634.655762, accuracy: 0.995000\n",
      "step: 16000, loss: 3656.087402, accuracy: 0.985000\n",
      "step: 16100, loss: 3651.037109, accuracy: 0.985000\n",
      "step: 16200, loss: 3642.169189, accuracy: 0.990000\n",
      "step: 16300, loss: 3634.140625, accuracy: 0.995000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 16400, loss: 3643.336670, accuracy: 0.990000\n",
      "step: 16500, loss: 3633.609619, accuracy: 0.995000\n",
      "step: 16600, loss: 3637.854980, accuracy: 0.995000\n",
      "step: 16700, loss: 3636.092773, accuracy: 0.995000\n",
      "step: 16800, loss: 3635.599609, accuracy: 0.995000\n",
      "step: 16900, loss: 3651.202637, accuracy: 0.980000\n",
      "step: 17000, loss: 3637.325439, accuracy: 0.995000\n",
      "step: 17100, loss: 3634.209473, accuracy: 0.995000\n",
      "step: 17200, loss: 3641.181885, accuracy: 0.990000\n",
      "step: 17300, loss: 3640.472412, accuracy: 0.990000\n",
      "step: 17400, loss: 3639.344482, accuracy: 0.990000\n",
      "step: 17500, loss: 3632.027588, accuracy: 1.000000\n",
      "step: 17600, loss: 3628.349854, accuracy: 0.995000\n",
      "step: 17700, loss: 3634.875977, accuracy: 0.995000\n",
      "step: 17800, loss: 3634.601562, accuracy: 0.995000\n",
      "step: 17900, loss: 3662.620850, accuracy: 0.975000\n",
      "step: 18000, loss: 3630.666748, accuracy: 0.995000\n",
      "step: 18100, loss: 3637.973389, accuracy: 0.995000\n",
      "step: 18200, loss: 3638.937744, accuracy: 0.990000\n",
      "step: 18300, loss: 3635.480713, accuracy: 0.995000\n",
      "step: 18400, loss: 3644.075684, accuracy: 0.990000\n",
      "step: 18500, loss: 3633.291260, accuracy: 0.995000\n",
      "step: 18600, loss: 3630.496338, accuracy: 0.995000\n",
      "step: 18700, loss: 3640.631348, accuracy: 0.990000\n",
      "step: 18800, loss: 3636.660156, accuracy: 0.995000\n",
      "step: 18900, loss: 3636.168945, accuracy: 0.990000\n",
      "step: 19000, loss: 3636.745605, accuracy: 0.990000\n",
      "step: 19100, loss: 3636.151855, accuracy: 0.990000\n",
      "step: 19200, loss: 3631.908936, accuracy: 0.995000\n",
      "step: 19300, loss: 3639.468262, accuracy: 0.990000\n",
      "step: 19400, loss: 3632.687012, accuracy: 0.995000\n",
      "step: 19500, loss: 3623.889160, accuracy: 1.000000\n",
      "step: 19600, loss: 3631.303223, accuracy: 0.990000\n",
      "step: 19700, loss: 3636.588135, accuracy: 0.990000\n",
      "step: 19800, loss: 3645.267334, accuracy: 0.985000\n",
      "step: 19900, loss: 3623.227051, accuracy: 1.000000\n",
      "step: 20000, loss: 3627.814453, accuracy: 0.995000\n",
      "step: 20100, loss: 3624.133789, accuracy: 1.000000\n",
      "step: 20200, loss: 3635.640869, accuracy: 0.990000\n",
      "step: 20300, loss: 3621.099609, accuracy: 1.000000\n",
      "step: 20400, loss: 3619.547119, accuracy: 1.000000\n",
      "step: 20500, loss: 3651.851318, accuracy: 0.980000\n",
      "step: 20600, loss: 3621.882568, accuracy: 1.000000\n",
      "step: 20700, loss: 3653.525391, accuracy: 0.975000\n",
      "step: 20800, loss: 3628.532471, accuracy: 0.995000\n",
      "step: 20900, loss: 3629.814453, accuracy: 0.995000\n",
      "step: 21000, loss: 3639.686279, accuracy: 0.985000\n",
      "step: 21100, loss: 3642.305664, accuracy: 0.985000\n",
      "step: 21200, loss: 3622.271240, accuracy: 1.000000\n",
      "step: 21300, loss: 3634.777588, accuracy: 0.990000\n",
      "step: 21400, loss: 3628.291992, accuracy: 0.995000\n",
      "step: 21500, loss: 3642.186279, accuracy: 0.985000\n",
      "step: 21600, loss: 3629.206299, accuracy: 0.995000\n",
      "step: 21700, loss: 3634.053711, accuracy: 0.990000\n",
      "step: 21800, loss: 3619.633301, accuracy: 1.000000\n",
      "step: 21900, loss: 3619.258545, accuracy: 1.000000\n",
      "step: 22000, loss: 3620.291992, accuracy: 1.000000\n",
      "step: 22100, loss: 3628.004150, accuracy: 0.995000\n",
      "step: 22200, loss: 3618.674561, accuracy: 1.000000\n",
      "step: 22300, loss: 3624.656982, accuracy: 0.995000\n",
      "step: 22400, loss: 3626.089600, accuracy: 0.995000\n",
      "step: 22500, loss: 3633.386963, accuracy: 0.990000\n",
      "step: 22600, loss: 3617.696045, accuracy: 1.000000\n",
      "step: 22700, loss: 3620.083496, accuracy: 1.000000\n",
      "step: 22800, loss: 3638.634277, accuracy: 0.985000\n",
      "step: 22900, loss: 3630.680664, accuracy: 0.990000\n",
      "step: 23000, loss: 3623.277832, accuracy: 0.995000\n",
      "step: 23100, loss: 3632.007812, accuracy: 0.990000\n",
      "step: 23200, loss: 3637.012207, accuracy: 0.990000\n",
      "step: 23300, loss: 3631.878174, accuracy: 0.990000\n",
      "step: 23400, loss: 3616.342529, accuracy: 1.000000\n",
      "step: 23500, loss: 3632.707275, accuracy: 0.990000\n",
      "step: 23600, loss: 3636.936768, accuracy: 0.985000\n",
      "step: 23700, loss: 3617.495361, accuracy: 1.000000\n",
      "step: 23800, loss: 3617.794189, accuracy: 1.000000\n",
      "step: 23900, loss: 3617.265381, accuracy: 1.000000\n",
      "step: 24000, loss: 3622.503662, accuracy: 0.995000\n",
      "step: 24100, loss: 3625.124268, accuracy: 0.995000\n",
      "step: 24200, loss: 3627.794434, accuracy: 0.995000\n",
      "step: 24300, loss: 3626.634766, accuracy: 0.995000\n",
      "step: 24400, loss: 3621.625244, accuracy: 0.995000\n",
      "step: 24500, loss: 3632.694092, accuracy: 0.990000\n",
      "step: 24600, loss: 3629.710205, accuracy: 0.990000\n",
      "step: 24700, loss: 3635.718262, accuracy: 0.985000\n",
      "step: 24800, loss: 3623.750000, accuracy: 0.995000\n",
      "step: 24900, loss: 3639.805664, accuracy: 0.985000\n",
      "step: 25000, loss: 3618.366943, accuracy: 1.000000\n",
      "step: 25100, loss: 3622.949951, accuracy: 0.995000\n",
      "step: 25200, loss: 3615.064697, accuracy: 1.000000\n",
      "step: 25300, loss: 3623.366455, accuracy: 0.995000\n",
      "step: 25400, loss: 3614.817871, accuracy: 1.000000\n",
      "step: 25500, loss: 3621.825928, accuracy: 0.995000\n",
      "step: 25600, loss: 3629.751953, accuracy: 0.990000\n",
      "step: 25700, loss: 3617.984619, accuracy: 1.000000\n",
      "step: 25800, loss: 3614.995605, accuracy: 1.000000\n",
      "step: 25900, loss: 3622.800293, accuracy: 0.995000\n",
      "step: 26000, loss: 3622.130371, accuracy: 0.995000\n",
      "step: 26100, loss: 3622.123047, accuracy: 0.995000\n",
      "step: 26200, loss: 3615.124268, accuracy: 1.000000\n",
      "step: 26300, loss: 3623.597656, accuracy: 0.995000\n",
      "step: 26400, loss: 3629.180420, accuracy: 0.990000\n",
      "step: 26500, loss: 3614.785645, accuracy: 1.000000\n",
      "step: 26600, loss: 3623.263672, accuracy: 0.995000\n",
      "step: 26700, loss: 3614.058350, accuracy: 1.000000\n",
      "step: 26800, loss: 3627.684570, accuracy: 0.990000\n",
      "step: 26900, loss: 3630.921143, accuracy: 0.990000\n",
      "step: 27000, loss: 3621.531738, accuracy: 0.995000\n",
      "step: 27100, loss: 3615.179688, accuracy: 1.000000\n",
      "step: 27200, loss: 3621.535400, accuracy: 0.995000\n",
      "step: 27300, loss: 3615.038574, accuracy: 1.000000\n",
      "step: 27400, loss: 3621.301270, accuracy: 0.995000\n",
      "step: 27500, loss: 3614.418945, accuracy: 1.000000\n",
      "step: 27600, loss: 3613.917236, accuracy: 1.000000\n",
      "step: 27700, loss: 3615.015869, accuracy: 1.000000\n",
      "step: 27800, loss: 3622.724609, accuracy: 0.995000\n",
      "step: 27900, loss: 3621.499023, accuracy: 0.995000\n",
      "step: 28000, loss: 3613.766357, accuracy: 1.000000\n",
      "step: 28100, loss: 3613.185059, accuracy: 1.000000\n",
      "step: 28200, loss: 3621.992920, accuracy: 0.995000\n",
      "step: 28300, loss: 3620.433838, accuracy: 0.995000\n",
      "step: 28400, loss: 3614.025635, accuracy: 1.000000\n",
      "step: 28500, loss: 3613.260742, accuracy: 1.000000\n",
      "step: 28600, loss: 3620.637451, accuracy: 0.995000\n",
      "step: 28700, loss: 3613.590332, accuracy: 1.000000\n",
      "step: 28800, loss: 3627.552246, accuracy: 0.990000\n",
      "step: 28900, loss: 3613.286133, accuracy: 1.000000\n",
      "step: 29000, loss: 3638.377441, accuracy: 0.985000\n",
      "step: 29100, loss: 3613.000488, accuracy: 1.000000\n",
      "step: 29200, loss: 3613.652100, accuracy: 1.000000\n",
      "step: 29300, loss: 3613.169678, accuracy: 1.000000\n",
      "step: 29400, loss: 3618.993652, accuracy: 0.995000\n",
      "step: 29500, loss: 3624.005371, accuracy: 0.990000\n",
      "step: 29600, loss: 3613.491943, accuracy: 1.000000\n",
      "step: 29700, loss: 3618.952393, accuracy: 0.995000\n",
      "step: 29800, loss: 3612.917725, accuracy: 1.000000\n",
      "step: 29900, loss: 3614.394775, accuracy: 1.000000\n",
      "step: 30000, loss: 3613.206787, accuracy: 1.000000\n",
      "step: 30100, loss: 3620.562988, accuracy: 0.995000\n",
      "step: 30200, loss: 3613.162598, accuracy: 1.000000\n",
      "step: 30300, loss: 3612.277832, accuracy: 1.000000\n",
      "step: 30400, loss: 3613.782715, accuracy: 1.000000\n",
      "step: 30500, loss: 3612.583008, accuracy: 1.000000\n",
      "step: 30600, loss: 3625.258789, accuracy: 0.990000\n",
      "step: 30700, loss: 3612.920410, accuracy: 1.000000\n",
      "step: 30800, loss: 3612.737305, accuracy: 1.000000\n",
      "step: 30900, loss: 3618.666260, accuracy: 0.995000\n",
      "step: 31000, loss: 3612.360107, accuracy: 1.000000\n",
      "step: 31100, loss: 3611.423096, accuracy: 1.000000\n",
      "step: 31200, loss: 3619.170166, accuracy: 0.995000\n",
      "step: 31300, loss: 3617.100098, accuracy: 0.995000\n",
      "step: 31400, loss: 3612.169922, accuracy: 1.000000\n",
      "step: 31500, loss: 3620.101807, accuracy: 0.995000\n",
      "step: 31600, loss: 3612.447754, accuracy: 1.000000\n",
      "step: 31700, loss: 3627.143799, accuracy: 0.990000\n",
      "step: 31800, loss: 3611.185547, accuracy: 1.000000\n",
      "step: 31900, loss: 3612.399658, accuracy: 1.000000\n",
      "step: 32000, loss: 3619.162842, accuracy: 0.995000\n",
      "step: 32100, loss: 3611.372559, accuracy: 1.000000\n",
      "step: 32200, loss: 3611.210449, accuracy: 1.000000\n",
      "step: 32300, loss: 3626.427246, accuracy: 0.990000\n",
      "step: 32400, loss: 3618.978271, accuracy: 0.995000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 32500, loss: 3612.177734, accuracy: 1.000000\n",
      "step: 32600, loss: 3611.401367, accuracy: 1.000000\n",
      "step: 32700, loss: 3611.375244, accuracy: 1.000000\n",
      "step: 32800, loss: 3611.875488, accuracy: 1.000000\n",
      "step: 32900, loss: 3611.196045, accuracy: 1.000000\n",
      "step: 33000, loss: 3611.071533, accuracy: 1.000000\n",
      "step: 33100, loss: 3611.490723, accuracy: 1.000000\n",
      "step: 33200, loss: 3611.072510, accuracy: 1.000000\n",
      "step: 33300, loss: 3619.176270, accuracy: 0.995000\n",
      "step: 33400, loss: 3611.389404, accuracy: 1.000000\n",
      "step: 33500, loss: 3617.436035, accuracy: 0.995000\n",
      "step: 33600, loss: 3617.716064, accuracy: 0.995000\n",
      "step: 33700, loss: 3617.975098, accuracy: 0.995000\n",
      "step: 33800, loss: 3618.954590, accuracy: 0.995000\n",
      "step: 33900, loss: 3623.056641, accuracy: 0.990000\n",
      "step: 34000, loss: 3619.088379, accuracy: 0.995000\n",
      "step: 34100, loss: 3610.712402, accuracy: 1.000000\n",
      "step: 34200, loss: 3610.467773, accuracy: 1.000000\n",
      "step: 34300, loss: 3610.719482, accuracy: 1.000000\n",
      "step: 34400, loss: 3610.958740, accuracy: 1.000000\n",
      "step: 34500, loss: 3610.359375, accuracy: 1.000000\n",
      "step: 34600, loss: 3617.617432, accuracy: 0.995000\n",
      "step: 34700, loss: 3611.348633, accuracy: 1.000000\n",
      "step: 34800, loss: 3610.186279, accuracy: 1.000000\n",
      "step: 34900, loss: 3610.784424, accuracy: 1.000000\n",
      "step: 35000, loss: 3610.710449, accuracy: 1.000000\n",
      "step: 35100, loss: 3610.213867, accuracy: 1.000000\n",
      "step: 35200, loss: 3611.044678, accuracy: 1.000000\n",
      "step: 35300, loss: 3610.845947, accuracy: 1.000000\n",
      "step: 35400, loss: 3610.497559, accuracy: 1.000000\n",
      "step: 35500, loss: 3610.753174, accuracy: 1.000000\n",
      "step: 35600, loss: 3610.589355, accuracy: 1.000000\n",
      "step: 35700, loss: 3626.344238, accuracy: 0.990000\n",
      "step: 35800, loss: 3617.177490, accuracy: 0.995000\n",
      "step: 35900, loss: 3610.265381, accuracy: 1.000000\n",
      "step: 36000, loss: 3609.838623, accuracy: 1.000000\n",
      "step: 36100, loss: 3611.102539, accuracy: 1.000000\n",
      "step: 36200, loss: 3616.740234, accuracy: 0.995000\n",
      "step: 36300, loss: 3610.088867, accuracy: 1.000000\n",
      "step: 36400, loss: 3610.217773, accuracy: 1.000000\n",
      "step: 36500, loss: 3618.471436, accuracy: 0.995000\n",
      "step: 36600, loss: 3609.932373, accuracy: 1.000000\n",
      "step: 36700, loss: 3610.117920, accuracy: 1.000000\n",
      "step: 36800, loss: 3610.443115, accuracy: 1.000000\n",
      "step: 36900, loss: 3618.281738, accuracy: 0.995000\n",
      "step: 37000, loss: 3610.109863, accuracy: 1.000000\n",
      "step: 37100, loss: 3609.939697, accuracy: 1.000000\n",
      "step: 37200, loss: 3609.925049, accuracy: 1.000000\n",
      "step: 37300, loss: 3609.982422, accuracy: 1.000000\n",
      "step: 37400, loss: 3610.405518, accuracy: 1.000000\n",
      "step: 37500, loss: 3610.157715, accuracy: 1.000000\n",
      "step: 37600, loss: 3610.001465, accuracy: 1.000000\n",
      "step: 37700, loss: 3616.988525, accuracy: 0.995000\n",
      "step: 37800, loss: 3609.375244, accuracy: 1.000000\n",
      "step: 37900, loss: 3609.766846, accuracy: 1.000000\n",
      "step: 38000, loss: 3615.862549, accuracy: 0.995000\n",
      "step: 38100, loss: 3617.331787, accuracy: 0.995000\n",
      "step: 38200, loss: 3610.077637, accuracy: 1.000000\n",
      "step: 38300, loss: 3623.632568, accuracy: 0.990000\n",
      "step: 38400, loss: 3609.449463, accuracy: 1.000000\n",
      "step: 38500, loss: 3609.965820, accuracy: 1.000000\n",
      "step: 38600, loss: 3609.702637, accuracy: 1.000000\n",
      "step: 38700, loss: 3609.514893, accuracy: 1.000000\n",
      "step: 38800, loss: 3610.001953, accuracy: 1.000000\n",
      "step: 38900, loss: 3616.364746, accuracy: 0.995000\n",
      "step: 39000, loss: 3617.796631, accuracy: 0.995000\n",
      "step: 39100, loss: 3617.755859, accuracy: 0.995000\n",
      "step: 39200, loss: 3609.737061, accuracy: 1.000000\n",
      "step: 39300, loss: 3621.962646, accuracy: 0.990000\n",
      "step: 39400, loss: 3609.719727, accuracy: 1.000000\n",
      "step: 39500, loss: 3617.435547, accuracy: 0.995000\n",
      "step: 39600, loss: 3609.142334, accuracy: 1.000000\n",
      "step: 39700, loss: 3609.853516, accuracy: 1.000000\n",
      "step: 39800, loss: 3616.363037, accuracy: 0.995000\n",
      "step: 39900, loss: 3609.566895, accuracy: 1.000000\n",
      "step: 40000, loss: 3616.982666, accuracy: 0.995000\n",
      "step: 40100, loss: 3609.359131, accuracy: 1.000000\n",
      "step: 40200, loss: 3608.990967, accuracy: 1.000000\n",
      "step: 40300, loss: 3609.333252, accuracy: 1.000000\n",
      "step: 40400, loss: 3609.603271, accuracy: 1.000000\n",
      "step: 40500, loss: 3609.282959, accuracy: 1.000000\n",
      "step: 40600, loss: 3609.642822, accuracy: 1.000000\n",
      "step: 40700, loss: 3609.374268, accuracy: 1.000000\n",
      "step: 40800, loss: 3625.081055, accuracy: 0.990000\n",
      "step: 40900, loss: 3609.347900, accuracy: 1.000000\n",
      "step: 41000, loss: 3609.412109, accuracy: 1.000000\n",
      "step: 41100, loss: 3609.501221, accuracy: 1.000000\n",
      "step: 41200, loss: 3609.313721, accuracy: 1.000000\n",
      "step: 41300, loss: 3609.220459, accuracy: 1.000000\n",
      "step: 41400, loss: 3609.119873, accuracy: 1.000000\n",
      "step: 41500, loss: 3608.987793, accuracy: 1.000000\n",
      "step: 41600, loss: 3609.331543, accuracy: 1.000000\n",
      "step: 41700, loss: 3609.357178, accuracy: 1.000000\n",
      "step: 41800, loss: 3609.166748, accuracy: 1.000000\n",
      "step: 41900, loss: 3639.418945, accuracy: 0.980000\n",
      "step: 42000, loss: 3608.923340, accuracy: 1.000000\n",
      "step: 42100, loss: 3608.818604, accuracy: 1.000000\n",
      "step: 42200, loss: 3616.927490, accuracy: 0.995000\n",
      "step: 42300, loss: 3616.883301, accuracy: 0.995000\n",
      "step: 42400, loss: 3608.981445, accuracy: 1.000000\n",
      "step: 42500, loss: 3617.040771, accuracy: 0.995000\n",
      "step: 42600, loss: 3609.677490, accuracy: 1.000000\n",
      "step: 42700, loss: 3609.358887, accuracy: 1.000000\n",
      "step: 42800, loss: 3617.118408, accuracy: 0.995000\n",
      "step: 42900, loss: 3609.014893, accuracy: 1.000000\n",
      "step: 43000, loss: 3609.157715, accuracy: 1.000000\n",
      "step: 43100, loss: 3609.211670, accuracy: 1.000000\n",
      "step: 43200, loss: 3609.179932, accuracy: 1.000000\n",
      "step: 43300, loss: 3608.877930, accuracy: 1.000000\n",
      "step: 43400, loss: 3609.130127, accuracy: 1.000000\n",
      "step: 43500, loss: 3616.965820, accuracy: 0.995000\n",
      "step: 43600, loss: 3608.943359, accuracy: 1.000000\n",
      "step: 43700, loss: 3608.794189, accuracy: 1.000000\n",
      "step: 43800, loss: 3616.652588, accuracy: 0.995000\n",
      "step: 43900, loss: 3615.860107, accuracy: 0.995000\n",
      "step: 44000, loss: 3608.930908, accuracy: 1.000000\n",
      "step: 44100, loss: 3608.604980, accuracy: 1.000000\n",
      "step: 44200, loss: 3608.886230, accuracy: 1.000000\n",
      "step: 44300, loss: 3608.980957, accuracy: 1.000000\n",
      "step: 44400, loss: 3609.123535, accuracy: 1.000000\n",
      "step: 44500, loss: 3608.901367, accuracy: 1.000000\n",
      "step: 44600, loss: 3617.014648, accuracy: 0.995000\n",
      "step: 44700, loss: 3608.551025, accuracy: 1.000000\n",
      "step: 44800, loss: 3608.817383, accuracy: 1.000000\n",
      "step: 44900, loss: 3608.798584, accuracy: 1.000000\n",
      "step: 45000, loss: 3608.621582, accuracy: 1.000000\n",
      "step: 45100, loss: 3608.758789, accuracy: 1.000000\n",
      "step: 45200, loss: 3616.821777, accuracy: 0.995000\n",
      "step: 45300, loss: 3608.707520, accuracy: 1.000000\n",
      "step: 45400, loss: 3608.733398, accuracy: 1.000000\n",
      "step: 45500, loss: 3623.589844, accuracy: 0.990000\n",
      "step: 45600, loss: 3615.767090, accuracy: 0.995000\n",
      "step: 45700, loss: 3608.468506, accuracy: 1.000000\n",
      "step: 45800, loss: 3608.934814, accuracy: 1.000000\n",
      "step: 45900, loss: 3616.847656, accuracy: 0.995000\n",
      "step: 46000, loss: 3614.956055, accuracy: 0.995000\n",
      "step: 46100, loss: 3616.579346, accuracy: 0.995000\n",
      "step: 46200, loss: 3608.546875, accuracy: 1.000000\n",
      "step: 46300, loss: 3608.667969, accuracy: 1.000000\n",
      "step: 46400, loss: 3608.647705, accuracy: 1.000000\n",
      "step: 46500, loss: 3616.610352, accuracy: 0.995000\n",
      "step: 46600, loss: 3608.784912, accuracy: 1.000000\n",
      "step: 46700, loss: 3616.771240, accuracy: 0.995000\n",
      "step: 46800, loss: 3608.284668, accuracy: 1.000000\n",
      "step: 46900, loss: 3608.342529, accuracy: 1.000000\n",
      "step: 47000, loss: 3608.471191, accuracy: 1.000000\n",
      "step: 47100, loss: 3608.466064, accuracy: 1.000000\n",
      "step: 47200, loss: 3608.617432, accuracy: 1.000000\n",
      "step: 47300, loss: 3608.440430, accuracy: 1.000000\n",
      "step: 47400, loss: 3608.470215, accuracy: 1.000000\n",
      "step: 47500, loss: 3608.465332, accuracy: 1.000000\n",
      "step: 47600, loss: 3608.433350, accuracy: 1.000000\n",
      "step: 47700, loss: 3608.708252, accuracy: 1.000000\n",
      "step: 47800, loss: 3615.334473, accuracy: 0.995000\n",
      "step: 47900, loss: 3608.516602, accuracy: 1.000000\n",
      "step: 48000, loss: 3608.361572, accuracy: 1.000000\n",
      "step: 48100, loss: 3615.632812, accuracy: 0.995000\n",
      "step: 48200, loss: 3608.668945, accuracy: 1.000000\n",
      "step: 48300, loss: 3608.395264, accuracy: 1.000000\n",
      "step: 48400, loss: 3608.519531, accuracy: 1.000000\n",
      "step: 48500, loss: 3608.613281, accuracy: 1.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 48600, loss: 3608.383301, accuracy: 1.000000\n",
      "step: 48700, loss: 3608.726807, accuracy: 1.000000\n",
      "step: 48800, loss: 3608.356445, accuracy: 1.000000\n",
      "step: 48900, loss: 3608.314453, accuracy: 1.000000\n",
      "step: 49000, loss: 3608.466553, accuracy: 1.000000\n",
      "step: 49100, loss: 3608.833740, accuracy: 1.000000\n",
      "step: 49200, loss: 3608.346924, accuracy: 1.000000\n",
      "step: 49300, loss: 3608.431152, accuracy: 1.000000\n",
      "step: 49400, loss: 3608.458496, accuracy: 1.000000\n",
      "step: 49500, loss: 3608.521729, accuracy: 1.000000\n",
      "step: 49600, loss: 3616.447021, accuracy: 0.995000\n",
      "step: 49700, loss: 3614.915039, accuracy: 0.995000\n",
      "step: 49800, loss: 3608.458740, accuracy: 1.000000\n",
      "step: 49900, loss: 3608.510010, accuracy: 1.000000\n",
      "step: 50000, loss: 3608.552490, accuracy: 1.000000\n",
      "step: 50100, loss: 3608.266602, accuracy: 1.000000\n",
      "step: 50200, loss: 3608.439941, accuracy: 1.000000\n",
      "step: 50300, loss: 3608.398193, accuracy: 1.000000\n",
      "step: 50400, loss: 3615.184326, accuracy: 0.995000\n",
      "step: 50500, loss: 3608.343262, accuracy: 1.000000\n",
      "step: 50600, loss: 3608.442383, accuracy: 1.000000\n",
      "step: 50700, loss: 3608.175293, accuracy: 1.000000\n",
      "step: 50800, loss: 3608.305420, accuracy: 1.000000\n",
      "step: 50900, loss: 3608.571289, accuracy: 1.000000\n",
      "step: 51000, loss: 3616.313232, accuracy: 0.995000\n",
      "step: 51100, loss: 3608.226074, accuracy: 1.000000\n",
      "step: 51200, loss: 3616.423096, accuracy: 0.995000\n",
      "step: 51300, loss: 3608.265625, accuracy: 1.000000\n",
      "step: 51400, loss: 3608.372559, accuracy: 1.000000\n",
      "step: 51500, loss: 3608.545654, accuracy: 1.000000\n",
      "step: 51600, loss: 3608.313477, accuracy: 1.000000\n",
      "step: 51700, loss: 3608.279053, accuracy: 1.000000\n",
      "step: 51800, loss: 3622.738037, accuracy: 0.990000\n",
      "step: 51900, loss: 3616.179932, accuracy: 0.995000\n",
      "step: 52000, loss: 3608.312988, accuracy: 1.000000\n",
      "step: 52100, loss: 3608.427734, accuracy: 1.000000\n",
      "step: 52200, loss: 3608.243652, accuracy: 1.000000\n",
      "step: 52300, loss: 3615.182129, accuracy: 0.995000\n",
      "step: 52400, loss: 3608.703369, accuracy: 1.000000\n",
      "step: 52500, loss: 3608.221680, accuracy: 1.000000\n",
      "step: 52600, loss: 3616.220459, accuracy: 0.995000\n",
      "step: 52700, loss: 3608.322998, accuracy: 1.000000\n",
      "step: 52800, loss: 3608.145508, accuracy: 1.000000\n",
      "step: 52900, loss: 3608.213379, accuracy: 1.000000\n",
      "step: 53000, loss: 3608.195312, accuracy: 1.000000\n",
      "step: 53100, loss: 3608.171631, accuracy: 1.000000\n",
      "step: 53200, loss: 3608.135254, accuracy: 1.000000\n",
      "step: 53300, loss: 3608.237305, accuracy: 1.000000\n",
      "step: 53400, loss: 3608.244873, accuracy: 1.000000\n",
      "step: 53500, loss: 3608.213135, accuracy: 1.000000\n",
      "step: 53600, loss: 3616.131348, accuracy: 0.995000\n",
      "step: 53700, loss: 3608.207275, accuracy: 1.000000\n",
      "step: 53800, loss: 3614.585205, accuracy: 0.995000\n",
      "step: 53900, loss: 3608.234375, accuracy: 1.000000\n",
      "step: 54000, loss: 3608.190186, accuracy: 1.000000\n",
      "step: 54100, loss: 3608.200195, accuracy: 1.000000\n",
      "step: 54200, loss: 3608.183350, accuracy: 1.000000\n",
      "step: 54300, loss: 3608.052490, accuracy: 1.000000\n",
      "step: 54400, loss: 3616.143311, accuracy: 0.995000\n",
      "step: 54500, loss: 3608.112793, accuracy: 1.000000\n",
      "step: 54600, loss: 3608.156982, accuracy: 1.000000\n",
      "step: 54700, loss: 3615.068848, accuracy: 0.995000\n",
      "step: 54800, loss: 3608.136963, accuracy: 1.000000\n",
      "step: 54900, loss: 3608.177002, accuracy: 1.000000\n",
      "step: 55000, loss: 3608.204834, accuracy: 1.000000\n",
      "step: 55100, loss: 3608.156250, accuracy: 1.000000\n",
      "step: 55200, loss: 3608.011230, accuracy: 1.000000\n",
      "step: 55300, loss: 3608.493164, accuracy: 1.000000\n",
      "step: 55400, loss: 3608.234863, accuracy: 1.000000\n",
      "step: 55500, loss: 3624.062256, accuracy: 0.990000\n",
      "step: 55600, loss: 3608.314453, accuracy: 1.000000\n",
      "step: 55700, loss: 3608.135742, accuracy: 1.000000\n",
      "step: 55800, loss: 3608.015381, accuracy: 1.000000\n",
      "step: 55900, loss: 3608.170166, accuracy: 1.000000\n",
      "step: 56000, loss: 3608.066406, accuracy: 1.000000\n",
      "step: 56100, loss: 3608.015137, accuracy: 1.000000\n",
      "step: 56200, loss: 3608.030762, accuracy: 1.000000\n",
      "step: 56300, loss: 3608.127197, accuracy: 1.000000\n",
      "step: 56400, loss: 3608.030273, accuracy: 1.000000\n",
      "step: 56500, loss: 3608.128418, accuracy: 1.000000\n",
      "step: 56600, loss: 3608.066162, accuracy: 1.000000\n",
      "step: 56700, loss: 3608.083984, accuracy: 1.000000\n",
      "step: 56800, loss: 3608.087646, accuracy: 1.000000\n",
      "step: 56900, loss: 3616.107666, accuracy: 0.995000\n",
      "step: 57000, loss: 3614.646484, accuracy: 0.995000\n",
      "step: 57100, loss: 3607.986572, accuracy: 1.000000\n",
      "step: 57200, loss: 3608.000977, accuracy: 1.000000\n",
      "step: 57300, loss: 3607.933350, accuracy: 1.000000\n",
      "step: 57400, loss: 3608.062012, accuracy: 1.000000\n",
      "step: 57500, loss: 3608.058350, accuracy: 1.000000\n",
      "step: 57600, loss: 3607.974609, accuracy: 1.000000\n",
      "step: 57700, loss: 3614.928467, accuracy: 0.995000\n",
      "step: 57800, loss: 3608.110352, accuracy: 1.000000\n",
      "step: 57900, loss: 3608.067871, accuracy: 1.000000\n",
      "step: 58000, loss: 3614.828857, accuracy: 0.995000\n",
      "step: 58100, loss: 3608.088867, accuracy: 1.000000\n",
      "step: 58200, loss: 3607.971680, accuracy: 1.000000\n",
      "step: 58300, loss: 3608.075684, accuracy: 1.000000\n",
      "step: 58400, loss: 3608.128418, accuracy: 1.000000\n",
      "step: 58500, loss: 3607.914062, accuracy: 1.000000\n",
      "step: 58600, loss: 3608.132812, accuracy: 1.000000\n",
      "step: 58700, loss: 3615.963379, accuracy: 0.995000\n",
      "step: 58800, loss: 3608.066895, accuracy: 1.000000\n",
      "step: 58900, loss: 3608.291016, accuracy: 1.000000\n",
      "step: 59000, loss: 3608.033447, accuracy: 1.000000\n",
      "step: 59100, loss: 3607.898193, accuracy: 1.000000\n",
      "step: 59200, loss: 3608.079102, accuracy: 1.000000\n",
      "step: 59300, loss: 3608.214355, accuracy: 1.000000\n",
      "step: 59400, loss: 3607.886719, accuracy: 1.000000\n",
      "step: 59500, loss: 3607.979004, accuracy: 1.000000\n",
      "step: 59600, loss: 3608.034424, accuracy: 1.000000\n",
      "step: 59700, loss: 3614.339111, accuracy: 0.995000\n",
      "step: 59800, loss: 3615.810547, accuracy: 0.995000\n",
      "step: 59900, loss: 3608.107910, accuracy: 1.000000\n",
      "step: 60000, loss: 3607.995605, accuracy: 1.000000\n",
      "step: 60100, loss: 3608.021240, accuracy: 1.000000\n",
      "step: 60200, loss: 3608.000732, accuracy: 1.000000\n",
      "step: 60300, loss: 3615.887939, accuracy: 0.995000\n",
      "step: 60400, loss: 3608.020020, accuracy: 1.000000\n",
      "step: 60500, loss: 3608.031982, accuracy: 1.000000\n",
      "step: 60600, loss: 3615.952637, accuracy: 0.995000\n",
      "step: 60700, loss: 3607.999756, accuracy: 1.000000\n",
      "step: 60800, loss: 3607.938721, accuracy: 1.000000\n",
      "step: 60900, loss: 3607.970459, accuracy: 1.000000\n",
      "step: 61000, loss: 3607.956787, accuracy: 1.000000\n",
      "step: 61100, loss: 3607.975098, accuracy: 1.000000\n",
      "step: 61200, loss: 3607.920166, accuracy: 1.000000\n",
      "step: 61300, loss: 3607.937012, accuracy: 1.000000\n",
      "step: 61400, loss: 3608.034180, accuracy: 1.000000\n",
      "step: 61500, loss: 3608.052246, accuracy: 1.000000\n",
      "step: 61600, loss: 3607.889648, accuracy: 1.000000\n",
      "step: 61700, loss: 3607.964111, accuracy: 1.000000\n",
      "step: 61800, loss: 3608.185791, accuracy: 1.000000\n",
      "step: 61900, loss: 3607.925781, accuracy: 1.000000\n",
      "step: 62000, loss: 3608.202881, accuracy: 1.000000\n",
      "step: 62100, loss: 3607.885742, accuracy: 1.000000\n",
      "step: 62200, loss: 3607.884521, accuracy: 1.000000\n",
      "step: 62300, loss: 3607.960449, accuracy: 1.000000\n",
      "step: 62400, loss: 3607.883789, accuracy: 1.000000\n",
      "step: 62500, loss: 3607.885254, accuracy: 1.000000\n",
      "step: 62600, loss: 3607.953125, accuracy: 1.000000\n",
      "step: 62700, loss: 3607.933350, accuracy: 1.000000\n",
      "step: 62800, loss: 3607.884766, accuracy: 1.000000\n",
      "step: 62900, loss: 3607.958984, accuracy: 1.000000\n",
      "step: 63000, loss: 3615.821045, accuracy: 0.995000\n",
      "step: 63100, loss: 3607.867188, accuracy: 1.000000\n",
      "step: 63200, loss: 3607.957764, accuracy: 1.000000\n",
      "step: 63300, loss: 3607.829346, accuracy: 1.000000\n",
      "step: 63400, loss: 3607.882812, accuracy: 1.000000\n",
      "step: 63500, loss: 3607.998291, accuracy: 1.000000\n",
      "step: 63600, loss: 3607.825684, accuracy: 1.000000\n",
      "step: 63700, loss: 3608.069336, accuracy: 1.000000\n",
      "step: 63800, loss: 3607.920410, accuracy: 1.000000\n",
      "step: 63900, loss: 3607.858643, accuracy: 1.000000\n",
      "step: 64000, loss: 3607.979980, accuracy: 1.000000\n",
      "step: 64100, loss: 3607.930176, accuracy: 1.000000\n",
      "step: 64200, loss: 3607.834717, accuracy: 1.000000\n",
      "step: 64300, loss: 3607.844482, accuracy: 1.000000\n",
      "step: 64400, loss: 3607.840820, accuracy: 1.000000\n",
      "step: 64500, loss: 3607.874023, accuracy: 1.000000\n",
      "step: 64600, loss: 3614.741943, accuracy: 0.995000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 64700, loss: 3607.885986, accuracy: 1.000000\n",
      "step: 64800, loss: 3607.837891, accuracy: 1.000000\n",
      "step: 64900, loss: 3607.777344, accuracy: 1.000000\n",
      "step: 65000, loss: 3615.825928, accuracy: 0.995000\n",
      "step: 65100, loss: 3607.853027, accuracy: 1.000000\n",
      "step: 65200, loss: 3607.845947, accuracy: 1.000000\n",
      "step: 65300, loss: 3607.840576, accuracy: 1.000000\n",
      "step: 65400, loss: 3607.835205, accuracy: 1.000000\n",
      "step: 65500, loss: 3607.916992, accuracy: 1.000000\n",
      "step: 65600, loss: 3607.922852, accuracy: 1.000000\n",
      "step: 65700, loss: 3607.744141, accuracy: 1.000000\n",
      "step: 65800, loss: 3607.779297, accuracy: 1.000000\n",
      "step: 65900, loss: 3607.908203, accuracy: 1.000000\n",
      "step: 66000, loss: 3615.798340, accuracy: 0.995000\n",
      "step: 66100, loss: 3607.783691, accuracy: 1.000000\n",
      "step: 66200, loss: 3607.964355, accuracy: 1.000000\n",
      "step: 66300, loss: 3607.752441, accuracy: 1.000000\n",
      "step: 66400, loss: 3607.865234, accuracy: 1.000000\n",
      "step: 66500, loss: 3607.978271, accuracy: 1.000000\n",
      "step: 66600, loss: 3607.857666, accuracy: 1.000000\n",
      "step: 66700, loss: 3607.856445, accuracy: 1.000000\n",
      "step: 66800, loss: 3607.833984, accuracy: 1.000000\n",
      "step: 66900, loss: 3607.875000, accuracy: 1.000000\n",
      "step: 67000, loss: 3607.866943, accuracy: 1.000000\n",
      "step: 67100, loss: 3607.841309, accuracy: 1.000000\n",
      "step: 67200, loss: 3614.211182, accuracy: 0.995000\n",
      "step: 67300, loss: 3607.791016, accuracy: 1.000000\n",
      "step: 67400, loss: 3607.821777, accuracy: 1.000000\n",
      "step: 67500, loss: 3607.809570, accuracy: 1.000000\n",
      "step: 67600, loss: 3607.790771, accuracy: 1.000000\n",
      "step: 67700, loss: 3607.784180, accuracy: 1.000000\n",
      "step: 67800, loss: 3607.819824, accuracy: 1.000000\n",
      "step: 67900, loss: 3607.801270, accuracy: 1.000000\n",
      "step: 68000, loss: 3607.776855, accuracy: 1.000000\n",
      "step: 68100, loss: 3607.814697, accuracy: 1.000000\n",
      "step: 68200, loss: 3608.098389, accuracy: 1.000000\n",
      "step: 68300, loss: 3615.880615, accuracy: 0.995000\n",
      "step: 68400, loss: 3607.756592, accuracy: 1.000000\n",
      "step: 68500, loss: 3607.937012, accuracy: 1.000000\n",
      "step: 68600, loss: 3607.823486, accuracy: 1.000000\n",
      "step: 68700, loss: 3607.797607, accuracy: 1.000000\n",
      "step: 68800, loss: 3607.842529, accuracy: 1.000000\n",
      "step: 68900, loss: 3607.792480, accuracy: 1.000000\n",
      "step: 69000, loss: 3607.808594, accuracy: 1.000000\n",
      "step: 69100, loss: 3607.888428, accuracy: 1.000000\n",
      "step: 69200, loss: 3607.750244, accuracy: 1.000000\n",
      "step: 69300, loss: 3607.741699, accuracy: 1.000000\n",
      "step: 69400, loss: 3607.765381, accuracy: 1.000000\n",
      "step: 69500, loss: 3607.778076, accuracy: 1.000000\n",
      "step: 69600, loss: 3614.158936, accuracy: 0.995000\n",
      "step: 69700, loss: 3607.731689, accuracy: 1.000000\n",
      "step: 69800, loss: 3607.786621, accuracy: 1.000000\n",
      "step: 69900, loss: 3607.734863, accuracy: 1.000000\n",
      "step: 70000, loss: 3623.723877, accuracy: 0.990000\n",
      "step: 70100, loss: 3607.869141, accuracy: 1.000000\n",
      "step: 70200, loss: 3607.759033, accuracy: 1.000000\n",
      "step: 70300, loss: 3607.932861, accuracy: 1.000000\n",
      "step: 70400, loss: 3607.790039, accuracy: 1.000000\n",
      "step: 70500, loss: 3607.697021, accuracy: 1.000000\n",
      "step: 70600, loss: 3607.771729, accuracy: 1.000000\n",
      "step: 70700, loss: 3615.727051, accuracy: 0.995000\n",
      "step: 70800, loss: 3607.698242, accuracy: 1.000000\n",
      "step: 70900, loss: 3607.762939, accuracy: 1.000000\n",
      "step: 71000, loss: 3607.830078, accuracy: 1.000000\n",
      "step: 71100, loss: 3607.771484, accuracy: 1.000000\n",
      "step: 71200, loss: 3607.826416, accuracy: 1.000000\n",
      "step: 71300, loss: 3607.778564, accuracy: 1.000000\n",
      "step: 71400, loss: 3607.791260, accuracy: 1.000000\n",
      "step: 71500, loss: 3607.783203, accuracy: 1.000000\n",
      "step: 71600, loss: 3614.071777, accuracy: 0.995000\n",
      "step: 71700, loss: 3607.713867, accuracy: 1.000000\n",
      "step: 71800, loss: 3607.697754, accuracy: 1.000000\n",
      "step: 71900, loss: 3607.797852, accuracy: 1.000000\n",
      "step: 72000, loss: 3607.775146, accuracy: 1.000000\n",
      "step: 72100, loss: 3607.993896, accuracy: 1.000000\n",
      "step: 72200, loss: 3607.762939, accuracy: 1.000000\n",
      "step: 72300, loss: 3607.726807, accuracy: 1.000000\n",
      "step: 72400, loss: 3607.846924, accuracy: 1.000000\n",
      "step: 72500, loss: 3607.758301, accuracy: 1.000000\n",
      "step: 72600, loss: 3607.720947, accuracy: 1.000000\n",
      "step: 72700, loss: 3607.759766, accuracy: 1.000000\n",
      "step: 72800, loss: 3607.715332, accuracy: 1.000000\n",
      "step: 72900, loss: 3607.764648, accuracy: 1.000000\n",
      "step: 73000, loss: 3607.802490, accuracy: 1.000000\n",
      "step: 73100, loss: 3607.787109, accuracy: 1.000000\n",
      "step: 73200, loss: 3623.702148, accuracy: 0.990000\n",
      "step: 73300, loss: 3607.762939, accuracy: 1.000000\n",
      "step: 73400, loss: 3607.773926, accuracy: 1.000000\n",
      "step: 73500, loss: 3608.172119, accuracy: 1.000000\n",
      "step: 73600, loss: 3608.132568, accuracy: 1.000000\n",
      "step: 73700, loss: 3607.826416, accuracy: 1.000000\n",
      "step: 73800, loss: 3615.711182, accuracy: 0.995000\n",
      "step: 73900, loss: 3607.763184, accuracy: 1.000000\n",
      "step: 74000, loss: 3607.729736, accuracy: 1.000000\n",
      "step: 74100, loss: 3607.655762, accuracy: 1.000000\n",
      "step: 74200, loss: 3607.786865, accuracy: 1.000000\n",
      "step: 74300, loss: 3607.781494, accuracy: 1.000000\n",
      "step: 74400, loss: 3607.719238, accuracy: 1.000000\n",
      "step: 74500, loss: 3607.955811, accuracy: 1.000000\n",
      "step: 74600, loss: 3607.772949, accuracy: 1.000000\n",
      "step: 74700, loss: 3607.674561, accuracy: 1.000000\n",
      "step: 74800, loss: 3607.718994, accuracy: 1.000000\n",
      "step: 74900, loss: 3607.820068, accuracy: 1.000000\n",
      "step: 75000, loss: 3607.759033, accuracy: 1.000000\n",
      "step: 75100, loss: 3607.767334, accuracy: 1.000000\n",
      "step: 75200, loss: 3607.731934, accuracy: 1.000000\n",
      "step: 75300, loss: 3614.077637, accuracy: 0.995000\n",
      "step: 75400, loss: 3607.761475, accuracy: 1.000000\n",
      "step: 75500, loss: 3607.743652, accuracy: 1.000000\n",
      "step: 75600, loss: 3607.696533, accuracy: 1.000000\n",
      "step: 75700, loss: 3607.827393, accuracy: 1.000000\n",
      "step: 75800, loss: 3607.791260, accuracy: 1.000000\n",
      "step: 75900, loss: 3607.750977, accuracy: 1.000000\n",
      "step: 76000, loss: 3607.738281, accuracy: 1.000000\n",
      "step: 76100, loss: 3607.682373, accuracy: 1.000000\n",
      "step: 76200, loss: 3607.691895, accuracy: 1.000000\n",
      "step: 76300, loss: 3607.762451, accuracy: 1.000000\n",
      "step: 76400, loss: 3607.777588, accuracy: 1.000000\n",
      "step: 76500, loss: 3607.707764, accuracy: 1.000000\n",
      "step: 76600, loss: 3607.732910, accuracy: 1.000000\n",
      "step: 76700, loss: 3607.761719, accuracy: 1.000000\n",
      "step: 76800, loss: 3607.645020, accuracy: 1.000000\n",
      "step: 76900, loss: 3607.809570, accuracy: 1.000000\n",
      "step: 77000, loss: 3607.707031, accuracy: 1.000000\n",
      "step: 77100, loss: 3607.769775, accuracy: 1.000000\n",
      "step: 77200, loss: 3607.720947, accuracy: 1.000000\n",
      "step: 77300, loss: 3615.741699, accuracy: 0.995000\n",
      "step: 77400, loss: 3607.718750, accuracy: 1.000000\n",
      "step: 77500, loss: 3607.814209, accuracy: 1.000000\n",
      "step: 77600, loss: 3607.693359, accuracy: 1.000000\n",
      "step: 77700, loss: 3607.768311, accuracy: 1.000000\n",
      "step: 77800, loss: 3607.901611, accuracy: 1.000000\n",
      "step: 77900, loss: 3607.698975, accuracy: 1.000000\n",
      "step: 78000, loss: 3607.754883, accuracy: 1.000000\n",
      "step: 78100, loss: 3607.814697, accuracy: 1.000000\n",
      "step: 78200, loss: 3607.738281, accuracy: 1.000000\n",
      "step: 78300, loss: 3607.797363, accuracy: 1.000000\n",
      "step: 78400, loss: 3607.726562, accuracy: 1.000000\n",
      "step: 78500, loss: 3607.747803, accuracy: 1.000000\n",
      "step: 78600, loss: 3607.715088, accuracy: 1.000000\n",
      "step: 78700, loss: 3607.680664, accuracy: 1.000000\n",
      "step: 78800, loss: 3607.732666, accuracy: 1.000000\n",
      "step: 78900, loss: 3607.693359, accuracy: 1.000000\n",
      "step: 79000, loss: 3607.705811, accuracy: 1.000000\n",
      "step: 79100, loss: 3607.728027, accuracy: 1.000000\n",
      "step: 79200, loss: 3607.704834, accuracy: 1.000000\n",
      "step: 79300, loss: 3607.791016, accuracy: 1.000000\n",
      "step: 79400, loss: 3607.713135, accuracy: 1.000000\n",
      "step: 79500, loss: 3607.738770, accuracy: 1.000000\n",
      "step: 79600, loss: 3607.812256, accuracy: 1.000000\n",
      "step: 79700, loss: 3607.753906, accuracy: 1.000000\n",
      "step: 79800, loss: 3615.680420, accuracy: 0.995000\n",
      "step: 79900, loss: 3607.694824, accuracy: 1.000000\n",
      "step: 80000, loss: 3607.709961, accuracy: 1.000000\n",
      "step: 80100, loss: 3607.719238, accuracy: 1.000000\n",
      "step: 80200, loss: 3607.710205, accuracy: 1.000000\n",
      "step: 80300, loss: 3607.780762, accuracy: 1.000000\n",
      "step: 80400, loss: 3607.684326, accuracy: 1.000000\n",
      "step: 80500, loss: 3607.706787, accuracy: 1.000000\n",
      "step: 80600, loss: 3607.721191, accuracy: 1.000000\n",
      "step: 80700, loss: 3607.715332, accuracy: 1.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 80800, loss: 3607.789062, accuracy: 1.000000\n",
      "step: 80900, loss: 3607.819336, accuracy: 1.000000\n",
      "step: 81000, loss: 3607.735840, accuracy: 1.000000\n",
      "step: 81100, loss: 3607.763916, accuracy: 1.000000\n",
      "step: 81200, loss: 3607.724365, accuracy: 1.000000\n",
      "step: 81300, loss: 3607.688477, accuracy: 1.000000\n",
      "step: 81400, loss: 3607.758545, accuracy: 1.000000\n",
      "step: 81500, loss: 3607.707275, accuracy: 1.000000\n",
      "step: 81600, loss: 3607.759521, accuracy: 1.000000\n",
      "step: 81700, loss: 3607.756348, accuracy: 1.000000\n",
      "step: 81800, loss: 3607.764893, accuracy: 1.000000\n",
      "step: 81900, loss: 3607.716553, accuracy: 1.000000\n",
      "step: 82000, loss: 3607.717041, accuracy: 1.000000\n",
      "step: 82100, loss: 3607.719971, accuracy: 1.000000\n",
      "step: 82200, loss: 3607.790527, accuracy: 1.000000\n",
      "step: 82300, loss: 3607.753662, accuracy: 1.000000\n",
      "step: 82400, loss: 3607.729248, accuracy: 1.000000\n",
      "step: 82500, loss: 3607.680420, accuracy: 1.000000\n",
      "step: 82600, loss: 3607.696533, accuracy: 1.000000\n",
      "step: 82700, loss: 3607.719727, accuracy: 1.000000\n",
      "step: 82800, loss: 3607.689209, accuracy: 1.000000\n",
      "step: 82900, loss: 3607.657227, accuracy: 1.000000\n",
      "step: 83000, loss: 3607.608398, accuracy: 1.000000\n",
      "step: 83100, loss: 3607.718994, accuracy: 1.000000\n",
      "step: 83200, loss: 3607.716553, accuracy: 1.000000\n",
      "step: 83300, loss: 3607.773926, accuracy: 1.000000\n",
      "step: 83400, loss: 3607.671143, accuracy: 1.000000\n",
      "step: 83500, loss: 3607.726074, accuracy: 1.000000\n",
      "step: 83600, loss: 3607.693604, accuracy: 1.000000\n",
      "step: 83700, loss: 3607.813965, accuracy: 1.000000\n",
      "step: 83800, loss: 3607.702393, accuracy: 1.000000\n",
      "step: 83900, loss: 3607.792725, accuracy: 1.000000\n",
      "step: 84000, loss: 3607.693848, accuracy: 1.000000\n",
      "step: 84100, loss: 3607.711182, accuracy: 1.000000\n",
      "step: 84200, loss: 3607.687500, accuracy: 1.000000\n",
      "step: 84300, loss: 3607.702637, accuracy: 1.000000\n",
      "step: 84400, loss: 3607.687500, accuracy: 1.000000\n",
      "step: 84500, loss: 3607.718262, accuracy: 1.000000\n",
      "step: 84600, loss: 3607.628418, accuracy: 1.000000\n",
      "step: 84700, loss: 3607.745850, accuracy: 1.000000\n",
      "step: 84800, loss: 3615.625977, accuracy: 0.995000\n",
      "step: 84900, loss: 3607.660645, accuracy: 1.000000\n",
      "step: 85000, loss: 3607.643311, accuracy: 1.000000\n",
      "step: 85100, loss: 3607.687012, accuracy: 1.000000\n",
      "step: 85200, loss: 3615.611572, accuracy: 0.995000\n",
      "step: 85300, loss: 3607.669678, accuracy: 1.000000\n",
      "step: 85400, loss: 3607.709473, accuracy: 1.000000\n",
      "step: 85500, loss: 3607.652344, accuracy: 1.000000\n",
      "step: 85600, loss: 3607.674805, accuracy: 1.000000\n",
      "step: 85700, loss: 3607.698730, accuracy: 1.000000\n",
      "step: 85800, loss: 3607.687988, accuracy: 1.000000\n",
      "step: 85900, loss: 3607.708496, accuracy: 1.000000\n",
      "step: 86000, loss: 3607.641846, accuracy: 1.000000\n",
      "step: 86100, loss: 3607.666992, accuracy: 1.000000\n",
      "step: 86200, loss: 3607.744385, accuracy: 1.000000\n",
      "step: 86300, loss: 3607.707031, accuracy: 1.000000\n",
      "step: 86400, loss: 3607.652832, accuracy: 1.000000\n",
      "step: 86500, loss: 3607.648926, accuracy: 1.000000\n",
      "step: 86600, loss: 3607.734863, accuracy: 1.000000\n",
      "step: 86700, loss: 3607.819824, accuracy: 1.000000\n",
      "step: 86800, loss: 3607.667480, accuracy: 1.000000\n",
      "step: 86900, loss: 3615.680664, accuracy: 0.995000\n",
      "step: 87000, loss: 3607.682617, accuracy: 1.000000\n",
      "step: 87100, loss: 3607.685547, accuracy: 1.000000\n",
      "step: 87200, loss: 3607.670166, accuracy: 1.000000\n",
      "step: 87300, loss: 3607.689453, accuracy: 1.000000\n",
      "step: 87400, loss: 3607.610352, accuracy: 1.000000\n",
      "step: 87500, loss: 3607.651855, accuracy: 1.000000\n",
      "step: 87600, loss: 3607.647949, accuracy: 1.000000\n",
      "step: 87700, loss: 3607.648193, accuracy: 1.000000\n",
      "step: 87800, loss: 3607.670166, accuracy: 1.000000\n",
      "step: 87900, loss: 3607.693359, accuracy: 1.000000\n",
      "step: 88000, loss: 3607.675049, accuracy: 1.000000\n",
      "step: 88100, loss: 3607.661133, accuracy: 1.000000\n",
      "step: 88200, loss: 3607.659180, accuracy: 1.000000\n",
      "step: 88300, loss: 3607.645996, accuracy: 1.000000\n",
      "step: 88400, loss: 3607.659180, accuracy: 1.000000\n",
      "step: 88500, loss: 3615.610596, accuracy: 0.995000\n",
      "step: 88600, loss: 3607.619141, accuracy: 1.000000\n",
      "step: 88700, loss: 3607.652100, accuracy: 1.000000\n",
      "step: 88800, loss: 3607.631592, accuracy: 1.000000\n",
      "step: 88900, loss: 3607.608643, accuracy: 1.000000\n",
      "step: 89000, loss: 3607.659180, accuracy: 1.000000\n",
      "step: 89100, loss: 3607.616943, accuracy: 1.000000\n",
      "step: 89200, loss: 3607.679443, accuracy: 1.000000\n",
      "step: 89300, loss: 3607.634277, accuracy: 1.000000\n",
      "step: 89400, loss: 3607.587891, accuracy: 1.000000\n",
      "step: 89500, loss: 3607.667236, accuracy: 1.000000\n",
      "step: 89600, loss: 3607.646240, accuracy: 1.000000\n",
      "step: 89700, loss: 3607.680176, accuracy: 1.000000\n",
      "step: 89800, loss: 3607.636230, accuracy: 1.000000\n",
      "step: 89900, loss: 3607.660889, accuracy: 1.000000\n",
      "step: 90000, loss: 3607.620850, accuracy: 1.000000\n",
      "step: 90100, loss: 3607.645264, accuracy: 1.000000\n",
      "step: 90200, loss: 3607.643555, accuracy: 1.000000\n",
      "step: 90300, loss: 3607.623047, accuracy: 1.000000\n",
      "step: 90400, loss: 3607.624268, accuracy: 1.000000\n",
      "step: 90500, loss: 3607.653320, accuracy: 1.000000\n",
      "step: 90600, loss: 3607.634033, accuracy: 1.000000\n",
      "step: 90700, loss: 3607.650146, accuracy: 1.000000\n",
      "step: 90800, loss: 3607.627686, accuracy: 1.000000\n",
      "step: 90900, loss: 3613.967773, accuracy: 0.995000\n",
      "step: 91000, loss: 3607.639648, accuracy: 1.000000\n",
      "step: 91100, loss: 3615.652588, accuracy: 0.995000\n",
      "step: 91200, loss: 3607.620117, accuracy: 1.000000\n",
      "step: 91300, loss: 3607.628906, accuracy: 1.000000\n",
      "step: 91400, loss: 3607.603760, accuracy: 1.000000\n",
      "step: 91500, loss: 3615.662109, accuracy: 0.995000\n",
      "step: 91600, loss: 3607.651367, accuracy: 1.000000\n",
      "step: 91700, loss: 3607.643555, accuracy: 1.000000\n",
      "step: 91800, loss: 3607.661377, accuracy: 1.000000\n",
      "step: 91900, loss: 3607.592285, accuracy: 1.000000\n",
      "step: 92000, loss: 3607.622314, accuracy: 1.000000\n",
      "step: 92100, loss: 3607.566895, accuracy: 1.000000\n",
      "step: 92200, loss: 3607.660156, accuracy: 1.000000\n",
      "step: 92300, loss: 3607.615723, accuracy: 1.000000\n",
      "step: 92400, loss: 3607.587402, accuracy: 1.000000\n",
      "step: 92500, loss: 3607.621094, accuracy: 1.000000\n",
      "step: 92600, loss: 3607.618164, accuracy: 1.000000\n",
      "step: 92700, loss: 3607.581299, accuracy: 1.000000\n",
      "step: 92800, loss: 3607.595703, accuracy: 1.000000\n",
      "step: 92900, loss: 3607.588623, accuracy: 1.000000\n",
      "step: 93000, loss: 3607.590332, accuracy: 1.000000\n",
      "step: 93100, loss: 3607.599121, accuracy: 1.000000\n",
      "step: 93200, loss: 3607.593018, accuracy: 1.000000\n",
      "step: 93300, loss: 3607.623535, accuracy: 1.000000\n",
      "step: 93400, loss: 3607.580322, accuracy: 1.000000\n",
      "step: 93500, loss: 3607.594971, accuracy: 1.000000\n",
      "step: 93600, loss: 3607.634766, accuracy: 1.000000\n",
      "step: 93700, loss: 3607.539795, accuracy: 1.000000\n",
      "step: 93800, loss: 3607.620117, accuracy: 1.000000\n",
      "step: 93900, loss: 3607.602539, accuracy: 1.000000\n",
      "step: 94000, loss: 3607.578125, accuracy: 1.000000\n",
      "step: 94100, loss: 3607.612793, accuracy: 1.000000\n",
      "step: 94200, loss: 3607.577881, accuracy: 1.000000\n",
      "step: 94300, loss: 3607.555176, accuracy: 1.000000\n",
      "step: 94400, loss: 3615.580566, accuracy: 0.995000\n",
      "step: 94500, loss: 3607.623291, accuracy: 1.000000\n",
      "step: 94600, loss: 3607.571777, accuracy: 1.000000\n",
      "step: 94700, loss: 3607.602539, accuracy: 1.000000\n",
      "step: 94800, loss: 3607.574707, accuracy: 1.000000\n",
      "step: 94900, loss: 3607.587891, accuracy: 1.000000\n",
      "step: 95000, loss: 3607.586914, accuracy: 1.000000\n",
      "step: 95100, loss: 3607.613281, accuracy: 1.000000\n",
      "step: 95200, loss: 3607.608154, accuracy: 1.000000\n",
      "step: 95300, loss: 3607.611816, accuracy: 1.000000\n",
      "step: 95400, loss: 3607.558105, accuracy: 1.000000\n",
      "step: 95500, loss: 3607.574707, accuracy: 1.000000\n",
      "step: 95600, loss: 3607.563232, accuracy: 1.000000\n",
      "step: 95700, loss: 3607.546387, accuracy: 1.000000\n",
      "step: 95800, loss: 3607.581543, accuracy: 1.000000\n",
      "step: 95900, loss: 3607.554199, accuracy: 1.000000\n",
      "step: 96000, loss: 3607.574707, accuracy: 1.000000\n",
      "step: 96100, loss: 3607.593994, accuracy: 1.000000\n",
      "step: 96200, loss: 3607.658447, accuracy: 1.000000\n",
      "step: 96300, loss: 3607.593262, accuracy: 1.000000\n",
      "step: 96400, loss: 3607.593750, accuracy: 1.000000\n",
      "step: 96500, loss: 3607.576172, accuracy: 1.000000\n",
      "step: 96600, loss: 3607.537842, accuracy: 1.000000\n",
      "step: 96700, loss: 3607.584229, accuracy: 1.000000\n",
      "step: 96800, loss: 3607.517334, accuracy: 1.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 96900, loss: 3607.552246, accuracy: 1.000000\n",
      "step: 97000, loss: 3607.564697, accuracy: 1.000000\n",
      "step: 97100, loss: 3607.649902, accuracy: 1.000000\n",
      "step: 97200, loss: 3615.574951, accuracy: 0.995000\n",
      "step: 97300, loss: 3607.528809, accuracy: 1.000000\n",
      "step: 97400, loss: 3607.527588, accuracy: 1.000000\n",
      "step: 97500, loss: 3607.578857, accuracy: 1.000000\n",
      "step: 97600, loss: 3607.569824, accuracy: 1.000000\n",
      "step: 97700, loss: 3613.864746, accuracy: 0.995000\n",
      "step: 97800, loss: 3607.534668, accuracy: 1.000000\n",
      "step: 97900, loss: 3607.547119, accuracy: 1.000000\n",
      "step: 98000, loss: 3607.583252, accuracy: 1.000000\n",
      "step: 98100, loss: 3607.572998, accuracy: 1.000000\n",
      "step: 98200, loss: 3607.563721, accuracy: 1.000000\n",
      "step: 98300, loss: 3607.574707, accuracy: 1.000000\n",
      "step: 98400, loss: 3607.578613, accuracy: 1.000000\n",
      "step: 98500, loss: 3607.563721, accuracy: 1.000000\n",
      "step: 98600, loss: 3607.560547, accuracy: 1.000000\n",
      "step: 98700, loss: 3607.580322, accuracy: 1.000000\n",
      "step: 98800, loss: 3607.529053, accuracy: 1.000000\n",
      "step: 98900, loss: 3607.554443, accuracy: 1.000000\n",
      "step: 99000, loss: 3607.570068, accuracy: 1.000000\n",
      "step: 99100, loss: 3607.557861, accuracy: 1.000000\n",
      "step: 99200, loss: 3607.570801, accuracy: 1.000000\n",
      "step: 99300, loss: 3607.546143, accuracy: 1.000000\n",
      "step: 99400, loss: 3607.566162, accuracy: 1.000000\n",
      "step: 99500, loss: 3607.515381, accuracy: 1.000000\n",
      "step: 99600, loss: 3607.612549, accuracy: 1.000000\n",
      "step: 99700, loss: 3607.575684, accuracy: 1.000000\n",
      "step: 99800, loss: 3607.584717, accuracy: 1.000000\n",
      "step: 99900, loss: 3607.546875, accuracy: 1.000000\n",
      "step: 100000, loss: 3607.592773, accuracy: 1.000000\n",
      "step: 100100, loss: 3607.604736, accuracy: 1.000000\n",
      "step: 100200, loss: 3607.537354, accuracy: 1.000000\n",
      "step: 100300, loss: 3607.560791, accuracy: 1.000000\n",
      "step: 100400, loss: 3607.593262, accuracy: 1.000000\n",
      "step: 100500, loss: 3607.532227, accuracy: 1.000000\n",
      "step: 100600, loss: 3607.600098, accuracy: 1.000000\n",
      "step: 100700, loss: 3607.568848, accuracy: 1.000000\n",
      "step: 100800, loss: 3607.587646, accuracy: 1.000000\n",
      "step: 100900, loss: 3607.570068, accuracy: 1.000000\n",
      "step: 101000, loss: 3607.577148, accuracy: 1.000000\n",
      "step: 101100, loss: 3607.542969, accuracy: 1.000000\n",
      "step: 101200, loss: 3613.791992, accuracy: 0.995000\n",
      "step: 101300, loss: 3607.599854, accuracy: 1.000000\n",
      "step: 101400, loss: 3607.527344, accuracy: 1.000000\n",
      "step: 101500, loss: 3607.529053, accuracy: 1.000000\n",
      "step: 101600, loss: 3613.838623, accuracy: 0.995000\n",
      "step: 101700, loss: 3607.537842, accuracy: 1.000000\n",
      "step: 101800, loss: 3607.514160, accuracy: 1.000000\n",
      "step: 101900, loss: 3607.527100, accuracy: 1.000000\n",
      "step: 102000, loss: 3607.595947, accuracy: 1.000000\n",
      "step: 102100, loss: 3607.559570, accuracy: 1.000000\n",
      "step: 102200, loss: 3607.532227, accuracy: 1.000000\n",
      "step: 102300, loss: 3607.573975, accuracy: 1.000000\n",
      "step: 102400, loss: 3607.500977, accuracy: 1.000000\n",
      "step: 102500, loss: 3607.557373, accuracy: 1.000000\n",
      "step: 102600, loss: 3607.521240, accuracy: 1.000000\n",
      "step: 102700, loss: 3607.539307, accuracy: 1.000000\n",
      "step: 102800, loss: 3607.526855, accuracy: 1.000000\n",
      "step: 102900, loss: 3607.549316, accuracy: 1.000000\n",
      "step: 103000, loss: 3607.525391, accuracy: 1.000000\n",
      "step: 103100, loss: 3607.580811, accuracy: 1.000000\n",
      "step: 103200, loss: 3607.492432, accuracy: 1.000000\n",
      "step: 103300, loss: 3607.575684, accuracy: 1.000000\n",
      "step: 103400, loss: 3607.543213, accuracy: 1.000000\n",
      "step: 103500, loss: 3607.591309, accuracy: 1.000000\n",
      "step: 103600, loss: 3607.529541, accuracy: 1.000000\n",
      "step: 103700, loss: 3607.517578, accuracy: 1.000000\n",
      "step: 103800, loss: 3607.533447, accuracy: 1.000000\n",
      "step: 103900, loss: 3607.559326, accuracy: 1.000000\n",
      "step: 104000, loss: 3607.544189, accuracy: 1.000000\n",
      "step: 104100, loss: 3607.541748, accuracy: 1.000000\n",
      "step: 104200, loss: 3607.560303, accuracy: 1.000000\n",
      "step: 104300, loss: 3607.512939, accuracy: 1.000000\n",
      "step: 104400, loss: 3607.508301, accuracy: 1.000000\n",
      "step: 104500, loss: 3607.547363, accuracy: 1.000000\n",
      "step: 104600, loss: 3607.509766, accuracy: 1.000000\n",
      "step: 104700, loss: 3607.471680, accuracy: 1.000000\n",
      "step: 104800, loss: 3607.487061, accuracy: 1.000000\n",
      "step: 104900, loss: 3607.549561, accuracy: 1.000000\n",
      "step: 105000, loss: 3607.539307, accuracy: 1.000000\n",
      "step: 105100, loss: 3607.496582, accuracy: 1.000000\n",
      "step: 105200, loss: 3607.564453, accuracy: 1.000000\n",
      "step: 105300, loss: 3607.525391, accuracy: 1.000000\n",
      "step: 105400, loss: 3607.566895, accuracy: 1.000000\n",
      "step: 105500, loss: 3607.572998, accuracy: 1.000000\n",
      "step: 105600, loss: 3607.504150, accuracy: 1.000000\n",
      "step: 105700, loss: 3607.530518, accuracy: 1.000000\n",
      "step: 105800, loss: 3607.545898, accuracy: 1.000000\n",
      "step: 105900, loss: 3607.478271, accuracy: 1.000000\n",
      "step: 106000, loss: 3607.528076, accuracy: 1.000000\n",
      "step: 106100, loss: 3607.521484, accuracy: 1.000000\n",
      "step: 106200, loss: 3607.497559, accuracy: 1.000000\n",
      "step: 106300, loss: 3607.526123, accuracy: 1.000000\n",
      "step: 106400, loss: 3607.529297, accuracy: 1.000000\n",
      "step: 106500, loss: 3607.517822, accuracy: 1.000000\n",
      "step: 106600, loss: 3607.493896, accuracy: 1.000000\n",
      "step: 106700, loss: 3607.572021, accuracy: 1.000000\n",
      "step: 106800, loss: 3613.843994, accuracy: 0.995000\n",
      "step: 106900, loss: 3607.525391, accuracy: 1.000000\n",
      "step: 107000, loss: 3607.578613, accuracy: 1.000000\n",
      "step: 107100, loss: 3607.538574, accuracy: 1.000000\n",
      "step: 107200, loss: 3607.560303, accuracy: 1.000000\n",
      "step: 107300, loss: 3607.582031, accuracy: 1.000000\n",
      "step: 107400, loss: 3613.842041, accuracy: 0.995000\n",
      "step: 107500, loss: 3607.470459, accuracy: 1.000000\n",
      "step: 107600, loss: 3607.552734, accuracy: 1.000000\n",
      "step: 107700, loss: 3607.492676, accuracy: 1.000000\n",
      "step: 107800, loss: 3607.478760, accuracy: 1.000000\n",
      "step: 107900, loss: 3607.508545, accuracy: 1.000000\n",
      "step: 108000, loss: 3607.553223, accuracy: 1.000000\n",
      "step: 108100, loss: 3607.495361, accuracy: 1.000000\n",
      "step: 108200, loss: 3607.612793, accuracy: 1.000000\n",
      "step: 108300, loss: 3607.486328, accuracy: 1.000000\n",
      "step: 108400, loss: 3607.507324, accuracy: 1.000000\n",
      "step: 108500, loss: 3612.085938, accuracy: 0.995000\n",
      "step: 108600, loss: 3607.464600, accuracy: 1.000000\n",
      "step: 108700, loss: 3607.510254, accuracy: 1.000000\n",
      "step: 108800, loss: 3607.679443, accuracy: 1.000000\n",
      "step: 108900, loss: 3607.498779, accuracy: 1.000000\n",
      "step: 109000, loss: 3607.507568, accuracy: 1.000000\n",
      "step: 109100, loss: 3607.627197, accuracy: 1.000000\n",
      "step: 109200, loss: 3607.505615, accuracy: 1.000000\n",
      "step: 109300, loss: 3607.532227, accuracy: 1.000000\n",
      "step: 109400, loss: 3607.503662, accuracy: 1.000000\n",
      "step: 109500, loss: 3607.498535, accuracy: 1.000000\n",
      "step: 109600, loss: 3607.521973, accuracy: 1.000000\n",
      "step: 109700, loss: 3607.545898, accuracy: 1.000000\n",
      "step: 109800, loss: 3607.485352, accuracy: 1.000000\n",
      "step: 109900, loss: 3607.518066, accuracy: 1.000000\n",
      "step: 110000, loss: 3607.492188, accuracy: 1.000000\n",
      "step: 110100, loss: 3607.533691, accuracy: 1.000000\n",
      "step: 110200, loss: 3607.502686, accuracy: 1.000000\n",
      "step: 110300, loss: 3615.509521, accuracy: 0.995000\n",
      "step: 110400, loss: 3607.469727, accuracy: 1.000000\n",
      "step: 110500, loss: 3607.479248, accuracy: 1.000000\n",
      "step: 110600, loss: 3607.492676, accuracy: 1.000000\n",
      "step: 110700, loss: 3615.465820, accuracy: 0.995000\n",
      "step: 110800, loss: 3607.536133, accuracy: 1.000000\n",
      "step: 110900, loss: 3607.640869, accuracy: 1.000000\n",
      "step: 111000, loss: 3607.515869, accuracy: 1.000000\n",
      "step: 111100, loss: 3607.494385, accuracy: 1.000000\n",
      "step: 111200, loss: 3607.490479, accuracy: 1.000000\n",
      "step: 111300, loss: 3607.465088, accuracy: 1.000000\n",
      "step: 111400, loss: 3607.443115, accuracy: 1.000000\n",
      "step: 111500, loss: 3607.578369, accuracy: 1.000000\n",
      "step: 111600, loss: 3607.496338, accuracy: 1.000000\n",
      "step: 111700, loss: 3607.494629, accuracy: 1.000000\n",
      "step: 111800, loss: 3607.471191, accuracy: 1.000000\n",
      "step: 111900, loss: 3607.520996, accuracy: 1.000000\n",
      "step: 112000, loss: 3607.526611, accuracy: 1.000000\n",
      "step: 112100, loss: 3607.574463, accuracy: 1.000000\n",
      "step: 112200, loss: 3607.517578, accuracy: 1.000000\n",
      "step: 112300, loss: 3607.473145, accuracy: 1.000000\n",
      "step: 112400, loss: 3607.513916, accuracy: 1.000000\n",
      "step: 112500, loss: 3607.551270, accuracy: 1.000000\n",
      "step: 112600, loss: 3607.461670, accuracy: 1.000000\n",
      "step: 112700, loss: 3607.549316, accuracy: 1.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 112800, loss: 3607.471436, accuracy: 1.000000\n",
      "step: 112900, loss: 3607.462646, accuracy: 1.000000\n",
      "step: 113000, loss: 3607.540039, accuracy: 1.000000\n",
      "step: 113100, loss: 3613.834229, accuracy: 0.995000\n",
      "step: 113200, loss: 3607.496826, accuracy: 1.000000\n",
      "step: 113300, loss: 3607.521973, accuracy: 1.000000\n",
      "step: 113400, loss: 3607.499268, accuracy: 1.000000\n",
      "step: 113500, loss: 3607.474121, accuracy: 1.000000\n",
      "step: 113600, loss: 3607.551025, accuracy: 1.000000\n",
      "step: 113700, loss: 3607.479492, accuracy: 1.000000\n",
      "step: 113800, loss: 3607.489502, accuracy: 1.000000\n",
      "step: 113900, loss: 3607.527588, accuracy: 1.000000\n",
      "step: 114000, loss: 3607.486328, accuracy: 1.000000\n",
      "step: 114100, loss: 3607.553223, accuracy: 1.000000\n",
      "step: 114200, loss: 3607.503174, accuracy: 1.000000\n",
      "step: 114300, loss: 3607.490723, accuracy: 1.000000\n",
      "step: 114400, loss: 3607.513184, accuracy: 1.000000\n",
      "step: 114500, loss: 3607.587158, accuracy: 1.000000\n",
      "step: 114600, loss: 3607.524658, accuracy: 1.000000\n",
      "step: 114700, loss: 3607.510498, accuracy: 1.000000\n",
      "step: 114800, loss: 3607.470215, accuracy: 1.000000\n",
      "step: 114900, loss: 3607.508301, accuracy: 1.000000\n",
      "step: 115000, loss: 3607.502686, accuracy: 1.000000\n",
      "step: 115100, loss: 3607.546875, accuracy: 1.000000\n",
      "step: 115200, loss: 3607.461670, accuracy: 1.000000\n",
      "step: 115300, loss: 3607.432129, accuracy: 1.000000\n",
      "step: 115400, loss: 3607.510010, accuracy: 1.000000\n",
      "step: 115500, loss: 3607.465088, accuracy: 1.000000\n",
      "step: 115600, loss: 3607.496338, accuracy: 1.000000\n",
      "step: 115700, loss: 3607.548340, accuracy: 1.000000\n",
      "step: 115800, loss: 3607.489502, accuracy: 1.000000\n",
      "step: 115900, loss: 3607.492432, accuracy: 1.000000\n",
      "step: 116000, loss: 3607.490967, accuracy: 1.000000\n",
      "step: 116100, loss: 3607.489746, accuracy: 1.000000\n",
      "step: 116200, loss: 3607.492676, accuracy: 1.000000\n",
      "step: 116300, loss: 3607.528076, accuracy: 1.000000\n",
      "step: 116400, loss: 3607.449463, accuracy: 1.000000\n",
      "step: 116500, loss: 3607.480957, accuracy: 1.000000\n",
      "step: 116600, loss: 3607.600098, accuracy: 1.000000\n",
      "step: 116700, loss: 3607.495605, accuracy: 1.000000\n",
      "step: 116800, loss: 3607.513184, accuracy: 1.000000\n",
      "step: 116900, loss: 3607.616699, accuracy: 1.000000\n",
      "step: 117000, loss: 3607.702148, accuracy: 1.000000\n",
      "step: 117100, loss: 3607.510986, accuracy: 1.000000\n",
      "step: 117200, loss: 3607.466064, accuracy: 1.000000\n",
      "step: 117300, loss: 3607.707520, accuracy: 1.000000\n",
      "step: 117400, loss: 3607.531738, accuracy: 1.000000\n",
      "step: 117500, loss: 3607.451660, accuracy: 1.000000\n",
      "step: 117600, loss: 3607.659180, accuracy: 1.000000\n",
      "step: 117700, loss: 3607.476562, accuracy: 1.000000\n",
      "step: 117800, loss: 3607.518066, accuracy: 1.000000\n",
      "step: 117900, loss: 3607.463135, accuracy: 1.000000\n",
      "step: 118000, loss: 3607.486816, accuracy: 1.000000\n",
      "step: 118100, loss: 3607.543701, accuracy: 1.000000\n",
      "step: 118200, loss: 3607.449463, accuracy: 1.000000\n",
      "step: 118300, loss: 3607.467529, accuracy: 1.000000\n",
      "step: 118400, loss: 3607.465820, accuracy: 1.000000\n",
      "step: 118500, loss: 3607.493408, accuracy: 1.000000\n",
      "step: 118600, loss: 3607.502930, accuracy: 1.000000\n",
      "step: 118700, loss: 3607.574219, accuracy: 1.000000\n",
      "step: 118800, loss: 3607.479004, accuracy: 1.000000\n",
      "step: 118900, loss: 3607.512939, accuracy: 1.000000\n",
      "step: 119000, loss: 3607.509277, accuracy: 1.000000\n",
      "step: 119100, loss: 3607.450928, accuracy: 1.000000\n",
      "step: 119200, loss: 3607.447021, accuracy: 1.000000\n",
      "step: 119300, loss: 3615.688965, accuracy: 0.995000\n",
      "step: 119400, loss: 3607.468506, accuracy: 1.000000\n",
      "step: 119500, loss: 3607.457520, accuracy: 1.000000\n",
      "step: 119600, loss: 3607.438477, accuracy: 1.000000\n",
      "step: 119700, loss: 3615.453369, accuracy: 0.995000\n",
      "step: 119800, loss: 3607.508789, accuracy: 1.000000\n",
      "step: 119900, loss: 3607.503662, accuracy: 1.000000\n",
      "step: 120000, loss: 3613.714355, accuracy: 0.995000\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of DNN-SVM on the testset is 0.9757000207901001\n"
     ]
    }
   ],
   "source": [
    "model = DNN_softmax_and_SVM.DNN_SVM(num_classes = 10,  \n",
    "                    num_input = 70,    \n",
    "                    n_hidden_1 = 512,  \n",
    "                    n_hidden_2 = 512,\n",
    "                   )\n",
    "step_list, loss_list, acc_list = model.fit( x_train, y_train, \n",
    "                                            optimizer = tf.optimizers.Adam(learning_rate = 0.001),\n",
    "                                            training_steps = 400*300, \n",
    "                                            display_step = 100, \n",
    "                                            batch_size = 200, \n",
    "                                          )\n",
    "\n",
    "\n",
    "plt.plot(step_list, acc_list )\n",
    "plt.title(f'model {model.name} on MNIST')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(step_list, loss_list )\n",
    "plt.title(f'model {model.name} on MNIST')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "model.evaluate(x_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion:\n",
    "The accuracy of DNN-softmax on the testset is 0.9696000218391418. The accuracy of DNN-SVM on the testset is 0.9749000072479248"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
